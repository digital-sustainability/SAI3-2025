{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e28ec73-c11c-46a0-b1b2-681b94c217ac",
   "metadata": {},
   "source": [
    "## Effect of Chunk Sizes on RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c1e09-4a39-442e-b5ed-90be8218e8df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The code assumes that the API key for OpenAI is already in the env variables. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdd87e-995a-4c48-bd51-39a067326420",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-openai\n",
    "!pip install pypdf\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c18f78-4ea0-43aa-96fa-5d41e30b0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188f185-2416-4b29-a8ca-63f2c3eff075",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"../datasets/10k/\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a16f1b-f1ab-43ab-9d7e-ff7cf4bda24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_documents = documents[:20]\n",
    "data_generator = DatasetGenerator.from_documents(eval_documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num = 5)\n",
    "\n",
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8b3c9-3e69-4997-903b-12572b5d4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use GPT-4 for evaluating the responses\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator()\n",
    "relevancy_gpt4 = RelevancyEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b9039-3e82-442a-aac6-1c915f0b55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_time_and_accuracy(eval_questions):\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents,\n",
    "    )\n",
    "\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f738cbb-9122-4496-8299-398afa2a44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_size in [128, 256, 512, 1024, 2048]:  # different chunk sizes\n",
    "    Settings.chunk_size = chunk_size\n",
    "    Settings.chunk_overlap = chunk_size*0.25\n",
    "    avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(eval_questions)\n",
    "    print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, \\\n",
    "            Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
