{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aea1048-2beb-4ce9-b3d9-57d0ef8e9de9",
   "metadata": {},
   "source": [
    "# Running local RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81e632-8b04-45e1-8548-63c24477e3d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    This notebook demonstrates running a RAG where the model is completely local. The model used here is DeepSeek.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79b49b-3b0f-46ed-a241-27ea287338dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-llama-cpp\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install gguf\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1b9e1-6bf1-4d98-85ab-8fc51d2f54ad",
   "metadata": {},
   "source": [
    "## Running llama-cpp docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90b03e-9b95-4bd5-ae9d-7223250f069b",
   "metadata": {},
   "source": [
    "Download the model and put it in the **deepseek** directory: https://drive.google.com/file/d/14pFGLH6hF2L20ILiSqyv145jONqHZPpA/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72d9be-fa1c-4562-a33e-c9aa1d25f358",
   "metadata": {},
   "source": [
    "`docker run -v ./deepseek:/models -p 8000:8000 ghcr.io/ggml-org/llama.cpp:server -m /models/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf --port 8000 --host 0.0.0.0 -n 1024`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64cb4c-b10f-426a-b4d1-ae6bab0f2475",
   "metadata": {},
   "source": [
    "## Querying the local model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa6300-27e6-43d7-9253-4606c87c62f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "You can use Openai API for interacting with llama.cpp servers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e528a-c17d-4225-9605-d11613b0332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\", # http://<Your api-server IP>:port\n",
    "    api_key = \"test\"  # set this in the UI\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "model=\"DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\",\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a limerick about python exceptions\"}\n",
    "]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca9aac-5cc6-4bf8-893d-d487f21b5850",
   "metadata": {},
   "source": [
    "## Combining llama-index with llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22040eea-b508-4d9b-bfa8-0c27b9ca788a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Creating a RAG using a local model.\n",
    "Please note that the model used here does not work with the server that you started and queried in the cells above.\n",
    "Here llama-index creates it's own version of llama.cpp model server internally.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f71095-8095-47c9-a6de-f1e3dc357364",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    If you would like to query the sever that is running in docker locally, you can use the Openai extension provided by llama-index.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593809e2-f602-4aa0-9e21-c5eb9f4c8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67f803-6b2f-4cff-8b02-7edb9678fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../datasets/paul_graham/\").load_data()\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\",\n",
    "                                          gguf_file=\"DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9e1b5-39f2-4bd3-b30f-23ae9340b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_to_prompt(messages):\n",
    "    messages = [{\"role\": m.role.value, \"content\": m.content} for m in messages]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    messages = [{\"role\": \"user\", \"content\": completion}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87552c9-6b8b-4b5b-a6e2-ad74854cd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=\"\",\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"./deepseek/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=16384,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c90eb0-7876-46c7-b04a-f47c4d41009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09d375-6a11-4d8e-85fe-4d1c435872f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
