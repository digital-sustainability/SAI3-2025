{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will discuss briefly the necessary methods to understand the Word2Vec algorithm. We will use Numpy for the basic computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a one-hot vector that selects the 3rd row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 0, 1, 0])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a matrix $A$ of four rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,   2,  3,  4],\n",
    "              [5,   6,  7,  8],\n",
    "              [9,  10, 11, 12],\n",
    "              [13, 14, 15, 16]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the column vector $x$ to select a row in matrix $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Dot-Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us simplify and assume that the dot-product of two vectors is the sum of the products of the scalars in the particular dimension of each vector, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u^T v = u \\cdot v = \\sum_{i=1}^n{u_i v_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more similar two vectors $u$ and $v$ are in terms of *directionality*, the larger the dot-product is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could think of the dot product $u \\cdot v$ as projecting one vector on the other. If the two vectors point into the same direction, the dot-product will be the largest. If one vector is orthogonal to the other, it will be $0$. If the vectors are pointing into opposite directions, the dot-product will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have some data or results of mutually exclusive variables that represent scores for an observation being of class $C = [ c_1, c_2, c_3 ]$, as represented by the columns in the vector $y$ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([4.0, 2.5, 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to convert the scores into a probability distribution that represents the likelihood that on the basis of these scores the observation belongs to one of the classes, we can compute the Softmax of the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(C_n) = \\frac{\\exp(\\theta \\cdot X_n)}{\\sum_{i=1}^N{\\exp(\\theta \\cdot X_i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\theta$ allows us to scale the results to increase the probabilities of lower scalars in the vector. The exponentiation of $X$ makes larger values much larger. If we include a parameter like $\\theta$, we can scale the effect and increase the probabilities assigned to lower values. See for more details the implementation of *softmax* below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python we can write this using Numpy's *exp* and *sum* functions. The *axis* parameter determines that the some is performed row-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax1(y):\n",
    "    return np.exp(y) / np.sum(np.exp(y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax1([4.0, 4.0, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide a parameter $\\theta$ to the function, to be able to scale the probability for low values up. The larger $\\theta$, the higher the probability assigned to lower values. We set the default for $\\theta$ to $1.0$ in the *softmax* definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y, t=1.0):\n",
    "    return np.exp(y / t) / np.sum(np.exp(y / t), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector of values $[ 4.0, 4.0, 2.0 ]$, we get the following probability distribution given a default $\\theta$ of $1.0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(np.array([4.0, 4.0, 2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we double $\\theta$, the probability assigned to the third scalar increases significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(np.array([4.0, 4.0, 2.0]), 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first focus on the skip-gram model. For every word $w$ at position $t$ with some window size or radius of $m$ we want to predict the context of $w$. Our *objective function* is to maximize the probability of any context word given the current center word:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J'(\\theta) = \\prod_{t=1}^T \\prod_{\\substack{-m \\leq j \\leq m\\\\j \\neq 0}} P(w_{t+j} | w_t; \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\theta$ here is a hyper-parameter. We will get back to it later. There is also another hidden hyper-parameter, that is the radius $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reformulate this equation as the sum of the log-likelihoods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T{ \\sum_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} \\log(P(w_{t+j}| w_t)) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the equation above we are averaging over all words in the text by dividing with $T$. We take the minus of the sums to get a positive score, since the log of a probability will be always negative. This way our goal is also to minimize the loss function $J(\\theta)$, that is to minimize the negative log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task can be described as: with a specific center word somewhere in the middle of a sentence, pick one random word in a specific radius of $n$ words left and right, we will get as output the probability for every word in the vocabulary of being the selected context word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using word pairs extracted from a text. We go word by word through some text, selecting this word as the center word, and pick the context words from a windows of $n$ size. For example, here for $n=3$ (left and right of the center word):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../illustrations/Sample_Generation_Word2vec.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be based on the distributional properties of the word pairs, that is the frequency of their cooccurrence. The left word in the example pairs above is the center word (red) and the right word is one of the context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have learned 300 features represented in vectors for center words and their context respectively, that is, we have two independent vectors for each word, one that represents its features when it is a context word, and one that represents its features as a center word. This is our model $p(w_{t+j}|w_t)$ for a word at position $t$ in the text, and words in $t+j$ positions in the text relative to $t$. For the purpose of explanation here, we could assume that we have such vector pairs of 300 features for 100,000 such words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following equation we will assume $c$ and $o$ to be indices in the space of the word types that we model. These indices now refer to the list of vocabulary, not to positions in the text, as $t$ does above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the Softmax of two word vectors for a context word (index $o$ in the vocabulary) and a center word (index $c$ in the vocabulary) by taking the dot-product of their 300-dimensional feature vectors. That is, one is the vector of a word in the context $u_o$, and the other is the vector of the center word $v_c$. The $u$ vectors are in the context word vectors and the $v$ vectors are the center word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{W=1}^V{\\exp(u_w^T v_c)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the dot-product between the one-hot vector representing one center word $w_t$ in the center word matrix $W$. This is the lookup matrix (looking up column) of word embedding matrix as representation of center word. (See Lecture 2, Word Vector Representations: word2vec, Stanford presentation by Chris Manning). As explained above, this will result in picking a concrete center word feature vector from the corresponding matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix}\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 1 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " -- & 0.2  & -- \\\\[0.3em]\n",
    " -- & -1.4 & -- \\\\[0.3em]\n",
    " -- & 0.3  & -- \\\\[0.3em]\n",
    " -- & -0.1 & -- \\\\[0.3em]\n",
    " -- &  0.1 & -- \\\\[0.3em]\n",
    " -- & -0.3 & -- \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    " 0.2 \\\\[0.3em]\n",
    " -1.4 \\\\[0.3em]\n",
    " 0.3 \\\\[0.3em]\n",
    " -0.1 \\\\[0.3em]\n",
    " 0.1 \\\\[0.3em]\n",
    " -0.3 \n",
    "\\end{bmatrix} = V_c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this center word matrix to be the **hidden layer** of a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take the dot-product of this vector of the center word with the matrix of the context vectors (here the scalars dashed out), or output word representation, for each word in the context (or the $n$-sized radius) around the center word, which gives us a 100,000 dimensional vector of weights for each word from the vocabulary in the context of the center vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_c \\cdot \\begin{bmatrix}\n",
    " -- & --  & -- \\\\[0.3em]\n",
    " -- & -- & -- \\\\[0.3em]\n",
    " -- & --  & -- \\\\[0.3em]\n",
    " -- & -- & -- \\\\[0.3em]\n",
    " -- & -- & -- \\\\[0.3em]\n",
    " -- & -- & -- \n",
    "\\end{bmatrix} = u_o \\cdot v_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this context vector matrix to be the **output matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we use *Softmax* to compute the probability distribution of this vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mbox{Softmax}(u_o \\cdot v_c) = \\mbox{Softmax}(\n",
    "\\begin{bmatrix}\n",
    " 1.7 \\\\[0.3em]\n",
    " 0.3 \\\\[0.3em]\n",
    " 0.1 \\\\[0.3em]\n",
    " -0.7 \\\\[0.3em]\n",
    " -0.2 \\\\[0.3em]\n",
    " 0.1 \\\\[0.3em]\n",
    " 0.7 \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    " 0.44 \\\\[0.3em]\n",
    " 0.11 \\\\[0.3em]\n",
    " 0.09 \\\\[0.3em]\n",
    " 0.04 \\\\[0.3em]\n",
    " 0.07 \\\\[0.3em]\n",
    " 0.09 \\\\[0.3em]\n",
    " 0.16 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here once more the softmax function over the vector $u_o \\cdot v_c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " softmax(np.array([1.7, 0.3, 0.1, -0.7, -0.2, 0.1, 0.7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rounded sum results in $1.0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([0.44, 0.11, 0.09, 0.04, 0.07, 0.09, 0.16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now compare the resulting probabilities with some ground truth and compute the error rate for example. If the ground truth for the above result would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix}\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 0 \\\\[0.3em]\n",
    " 1 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this would mean that the model assigned a probability of $0.16$ to this word, then we could compute the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we learn the parameters for each word that would maximize the prediction of a context word (or vice versa)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that all parameters of the model are defined in a long vector $\\theta$. This vector will have twice the length of the vocabulary size, containing a weight for every word as a center word, and every word as a context word:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta = \\begin{bmatrix}\n",
    " v_{a}    \\\\[0.3em]\n",
    " v_{ant}  \\\\[0.3em]\n",
    " \\vdots    \\\\[0.3em]\n",
    " v_{zero} \\\\[0.3em]\n",
    " u_{a}    \\\\[0.3em]\n",
    " u_{ant}  \\\\[0.3em]\n",
    " \\vdots  \\\\[0.3em]\n",
    " u_{zero}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{2dV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat here again the objective function in which we want to minimize the log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T{ \\sum_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} \\log(P(w_{t+j}| w_t)) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our softmax function discussed above fits into this equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(o|c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{W=1}^V{\\exp(u_w^T v_c)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T{ \\sum_{\\substack{-m \\leq j \\leq m\\\\ j \\neq 0}} \\log(p(o|c)) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to minimize the loss function and maximize the likelihood that we predict the right word in the context for any given center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the parameters can be achieved using the gradient. We want to minimize the negative log of the following equation, computing the partial derivative with respect to the center vector ($v_c$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial }{\\partial v_c} \\log(\\frac{\\exp(u_o^T v_c)}{\\sum_{W=1}^V{\\exp(u_w^T v_c)}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a partial derivative of a function with several variables is its derivative with respect to one of these variables. Here it is $v_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log of a division can be converted into a subtraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial }{\\partial v_c} \\  \\log(\\exp(u_o^T v_c)) - \\log(\\sum_{W=1}^V{\\exp(u_w^T v_c)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify the first part of the subtraction, since $\\log$ and $\\exp$ cancel each other out, and the partial derivative with respect to $v_c$ of the simplified equation is simply $u_0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial }{\\partial v_c} \\  \\log(\\exp(u_o^T v_c)) = \\frac{\\partial }{\\partial v_c} \\  u_o^T v_c = u_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the subtraction above, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial }{\\partial v_c} \\  \\log(\\sum_{W=1}^V{\\exp(u_w^T v_c)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule we can simplify this equation as well. The chain rule expresses the derivative of the composition of two functions, that is mapping $x$ onto $f(g(x))$, in terms of derivatives of $f$ and $g$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first start with derivatives of simple functions. What is the derivative of a constant $5$ for example? It is $0$. Since the derivative measures the change of a function, and a constant does not change, the derivative of a constant is $0$. The derivative of a variable $x$ is $1$. The derivative of the product of a constant and a variable is the constant. The derivative of $x^n$ is $n \\cdot x^{n-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A differentiable function of a variable is a function whose derivative exists at each point in its domain ([see here](https://en.wikipedia.org/wiki/Differentiable_function)).The graph of a differentiable function must have a (non-vertical) tangent line at each point in its domain, and cannot contain any breaks, bends, or cusps. The following function $y = |x|$ (absolute value function) is not differentiable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../illustrations/600px-Absolute_value.svg.png\" width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is differentiable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../illustrations/Polynomialdeg3.svg.png\" width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule allows one to find derivatives of compositions of functions by knowing the derivative of the elementary functions from the composition. Let $g(x)$ be differentiable at $x$ and $f(x)$ be differentiable at $f(g(x))$. Then, if $y=f(g(x))$ and $u=g(x)$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dy}{dx}=\\frac{dy}{du}\\cdot\\frac{du}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see for the derivation Chris Manning's video..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation he derives is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u_o - \\sum_{x=1}^V p(x|c) u_x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He labels the left part ($u_o$) as *observation*. This is the context word vectors that we identified in the texts or data. He labels the second part ($\\sum_{x=1}^V p(x|c) u_x$) as *expectation*. This is the part that we want to tweak such that the loss function is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For maximizing or minimizing the cost (or objective) function using gradient descent, consider this simple example: Find the local minimum of the function $f(x) = x^4 - 3 x^3 + 2$ with derivative $f'(x) = 4 x^3 - 9 x^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting a fraction of the gradient moves you to the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 0\n",
    "x_new = 6\n",
    "eps = 0.01 # step size\n",
    "precision = 0.00001\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    x_new = x_old - eps * f_derivative(x_old)\n",
    "    print(\"x_old:\", x_old, \" Local minimum occurs at\", x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../illustrations/Tangent-calculus.svg.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "<p><center><a href=\"https://en.wikipedia.org/wiki/Derivative\">Wikipedia Derivative</a></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the idea is to identify the gradient at point $x$, we subtract a little fraction of the gradient, which moves us downhill towards the minimum. We continue with computing the gradient again at this point and walking down towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimization of an objective function $J(\\theta)$ over the entire training corpus makes it necessary to compute gradients for all windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would need to update for each element of $\\theta$ to identify the derivatives of the objective function with respect to all the parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_j^{new} = \\theta_j^{old} - \\alpha \\frac{\\partial }{\\partial \\theta_j^{old}} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ is the step size in the Gradient Descent algorithm. We have some parameter values ($\\theta_j^{old}$) and we identified the gradient (the $\\frac{\\partial}{\\partial\\theta_j^{old}}$ portion) at this position ($j$). We subtract a fraction of the gradient from the old parameters and get new ones. If this gives us a lower objection value, this takes us towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix notation for all parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^{new} = \\theta^{old} - \\alpha \\frac{\\partial}{\\partial \\theta^{old}} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^{new} = \\theta^{old} - \\alpha \\nabla_\\theta J(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Gradient Descent code with some stopping condition to be added to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a pseudo-code and is not meant to be executed (will throw an error)\n",
    "while True:\n",
    "    theta_grad = evaluate_gradient(J, corpus, theta)\n",
    "    theta = theta - alpha * theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue lines show the contour lines of the value of the objective function. Computing the Gradient we identify the direction of the steepest descent. Walking in small steps down this line of the steepest descent takes us to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../illustrations/512px-Gradient_descent.svg.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "<p><center>(<a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">Wikipedia: Gradient Descent</a>)</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve walking continuously down towards the minimum, $\\alpha$ needs to be small enough so that one does not jump over the minimum to the other side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem would be that, if we have a billion token corpus, this might include a lot of computations and optimizations. Computing the gradient of the objective function for a very large corpus will take very long for even the first gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the gradient and optimize at one position $t$ in the corpus ($t$ is the index of the center word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^{new} = \\theta^{old} - \\alpha \\nabla_\\theta J_t(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a pseudo-code and is not meant to be executed (will throw an error)\n",
    "while True:\n",
    "    window = sample_window(corpus)\n",
    "    theta_grad = evaluate_gradient(J, window, theta)\n",
    "    theta = theta - alpha * theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gensim](https://radimrehurek.com/gensim/index.html) provides an [API documentation for word2vec](https://radimrehurek.com/gensim/models/word2vec.html). There is a [GitHub repository](https://github.com/RaRe-Technologies/w2v_server_googlenews) with the code for a high-performance similarity server using [Gensim](https://radimrehurek.com/gensim/index.html) and word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input for Gensim's word2vec has to be a list of sentences and sentences are a list of tokens. We will import *gensim* and train a model on some sample sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's word2vec allows you to prune the internal dictionary. This can eliminate tokens that occur a minimum number of times. The *min_count* parameter provides this restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim offers an API for:\n",
    "- evaluation using standard data sets and formats (e.g. Google test set)\n",
    "- storing and loading of models\n",
    "- online or resuming of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# load a pre-trained model\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vector for a word (\"king\" is used as an example)\n",
    "word2vec_model.get_vector(\"king\")  # you can also use word2vec_model[\"king\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic similarity example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic Word2Vec example\n",
    "In the word2vec vector space, **Queen** = **King** - **Man** + **Woman** (sort of!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Word2Vec Model with your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from multiprocessing import cpu_count\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text8 dataset\n",
    "dataset = api.load(\"text8\")  # you can replace this with your own dataset. If you do that, please remember to pre-process your dataset.\n",
    "\n",
    "# extract a list of words from the dataset\n",
    "data =[]\n",
    "for word in dataset:\n",
    "    data.append(word)\n",
    "\n",
    "# We will split the data into two parts\n",
    "data_train = data[:1200] # this is used to train the model\n",
    "data_update = data[1200:] # this part will be used to update the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "w2v_model = Word2Vec(data_train, min_count=0, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vector for the word \"time\"\n",
    "w2v_model.wv['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar words to the word \"time\"\n",
    "w2v_model.wv.most_similar('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model\n",
    "w2v_model.save('Word2VecModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Update the pre-trained model with some new dataset\n",
    "**data_update** is used here, you can use your own. Please remember to pre-process your own dataset e.g. cleaning, stemming etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your model\n",
    "model = Word2Vec.load('Word2VecModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model vocabulary from a sequence of sentences\n",
    "model.build_vocab(data_update, update = True)\n",
    "\n",
    "# train word vectors\n",
    "model.train(data_update, total_examples=model.corpus_count, epochs=1) # you can increase the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vector for the word \"time\" after the update over the original training\n",
    "model.wv['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar words to the word \"time\"\n",
    "w2v_model.wv.most_similar('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model\n",
    "model.save('Word2VecModelUpdated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Can you update the pre-trained model in **1.** using your own dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
