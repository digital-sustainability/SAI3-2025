{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSh_WANmJ1m4"
   },
   "source": [
    "# Pandas\n",
    "\n",
    "While all data can be reduced to numbers and therefore to Numpy arrays, very often data contain different types of information: numbers, text, dates etc. Therefore we need a way to import such data and organise them so that we can *then* do Machine Learning on them. The most popular library in Python for this is Pandas. We are presenting here only the bare minimum functionalities and will see more advanced aspects while we progress in the course.\n",
    "\n",
    "The main data structure that we will use in this course is the ```DataFrame```. It is a table of data with rows (indices) and columns where usually each row is an item of the dataset and the columns the various properties of that object. We will briefly mention later how to create Dataframes from scratch, but most commonly they are directly created when importing data via Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real dataset\n",
    "\n",
    "Instead of simply importing existing data, we will try to make here a more realistic example where you record data and then import them using Pandas. For that we will use the [phyphox app](https://phyphox.org/) which allows to use the phone's sensor to record various types of data. Here we use acceleration data (top row). If you want to acquire your own data:\n",
    "- download the app and open it\n",
    "- tap on Acceleration\n",
    "- use the play button to record and stop\n",
    "- use the three dots to export the data\n",
    "- pick your favourite format (here we use CSV)\n",
    "- send it to your email, download it to your computer and unzip it\n",
    "- copy the ```Raw data.csv``` file to the ```datasets``` folder and rename it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,8))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(imread(f'illustrations/phyphox{i+1}.png'))\n",
    "    ax[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataframes\n",
    "\n",
    "To read the data we generated, we use on of the many Pandas functions called ```pd.read_xxx```. Those are capable to read most of the popular table and database formats such as CSV, SQL, Excel etc. The file can be read either directly on disk or from an online repository. Here we read one of the accleretion datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running = pd.read_csv('../datasets/running.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly visualize the table using the ```head()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structures underlying DataFrames are actually Numpy arrays. Hence a large part of the features seen on arrays is similar here. For example we can get the size of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use logical operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJbgsCJ6Nprz"
   },
   "source": [
    "## Index and column\n",
    "\n",
    "One of the main differences between Numpy arrays and Pandas DataFrames is that the elements in the table are not located by their \"numerical position\" but by their corresponding index and column. These two elements are in fact just list of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1203,
     "status": "ok",
     "timestamp": 1614506291986,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "uGRwTSzUOpHe",
    "outputId": "8bc8fc12-0e68-4ed2-a682-876e27e86f48"
   },
   "outputs": [],
   "source": [
    "running.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1192,
     "status": "ok",
     "timestamp": 1614506291986,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "EV4IxYpRQqwm",
    "outputId": "4787b80d-2abe-4432-e412-03621b08d7f1"
   },
   "outputs": [],
   "source": [
    "running.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVMDzJlXOr6h"
   },
   "source": [
    "If needed, this row and column names can be changed. For example using the ```rename``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1614506291987,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "NHKd5z2lRbe-",
    "outputId": "9582b528-c269-41df-9ad8-3cac6101e2ec"
   },
   "outputs": [],
   "source": [
    "running = running.rename(columns={'Linear Acceleration x (m/s^2)': 'acc_x'})\n",
    "running.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's essential to re-assign the result to the ```running``` variable. Otherwise the update is not saved. Instead one can also use the ```inplace``` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.rename(columns={'Linear Acceleration y (m/s^2)': 'acc_y'}, inplace=True)\n",
    "running.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing values\n",
    "\n",
    "We have seen that with arrays we could use slicing of the form ```myarray[0:3, 5:8]``` to recover a smaller array with rows 1-2 and colums 5-7. With DataFrames we can also use rows and column numbers to create a sub-DataFrame, but we can also use directly the indices and column names for that. To clarify this distinction, one has to use the methods ```loc``` (rows and column names) and ```iloc``` (rows and column numbers). For example to recover the rows 1-3, and columns 1-2 we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.iloc[1:4, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover by name we can now use ```loc```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.loc[1:4, ['acc_x', 'acc_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full columns can also be extracted either via simple brackets or via the dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running['acc_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.acc_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we will need to access the underlying Numpy arrays. This can be done with the ```values``` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.acc_x.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, importantly we can also use logical indexing as we did for Numpy arrays. For example we can create a column filled with True/False values and use that column for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running.acc_x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running[running.acc_x > 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Ji98iJPLIy"
   },
   "source": [
    "## Adding columns\n",
    "\n",
    "It is also very easy to add a new column to an existing DataFrame. For that you just assign a list or a single value to a not yet existing column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1482,
     "status": "ok",
     "timestamp": 1614506292347,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "6uu7G5ZGPVk8",
    "outputId": "a4e46a85-2ef9-414f-808b-404d9538b6df"
   },
   "outputs": [],
   "source": [
    "running['new column'] = 1\n",
    "running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF3dSFFxRaGV"
   },
   "source": [
    "## Droping items\n",
    "Inversely we can remove columns and rows by using the ```drop()``` function. Again here, to get the modified version of ```running``` we have to re-assign it or use ```inplace```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1614506292351,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "ey3103aeS6qB",
    "outputId": "a568b9e0-7ec8-4b82-905d-8594c9619323"
   },
   "outputs": [],
   "source": [
    "running.drop(columns=['Time (s)'], inplace=True)\n",
    "running.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "executionInfo": {
     "elapsed": 1444,
     "status": "ok",
     "timestamp": 1614506292352,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "fAv6nXylTF6y",
    "outputId": "06bca125-16f8-4c83-f7af-746e435a06bd"
   },
   "outputs": [],
   "source": [
    "running.drop(index=[0,4], inplace=True)\n",
    "running.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p_8i8DnoCDb"
   },
   "source": [
    "## Importing data\n",
    "\n",
    "Until now we have always used the ```reader_XXX``` functions with default options. However these functions have many options, in particular regarding the choice of indices and column names. For example, we can specify if we want to use **one of the columns as index** when importing data. Each data format will present its own challenges, and we present here some core concepts.\n",
    "\n",
    "Here we are using the dataset that you can find [here](https://github.com/guiwitz/NumpyPandas_course/blob/master/Data/composers.xlsx). It is a simple collection of Excel sheet with information regarding classical composers. These simple and rather short dataset will allow us to explore many of the problems that you might encounter with \"real\" datasets.\n",
    "\n",
    "First we need a new importer function for Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 2652,
     "status": "ok",
     "timestamp": 1614506293582,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "fEE5cD20qkhE",
    "outputId": "f0a661cf-988f-4ee5-c5c0-f420b25e2650"
   },
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/guiwitz/NumpyPandas_course/blob/master/Data/composers.xlsx?raw=true'\n",
    "composers = pd.read_excel(file_url)\n",
    "composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PtIZ7x6qrjU"
   },
   "source": [
    "### Setting indices\n",
    "Again, by default Pandas sets a numerical index. However here each composer name is unique, so we could use that as an index. We can specify this to our importer with the ```index_col``` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2638,
     "status": "ok",
     "timestamp": 1614506293583,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "eB-tf_Caq89E",
    "outputId": "42436a62-0f28-479a-b980-d49a18a9bb40"
   },
   "outputs": [],
   "source": [
    "composers = pd.read_excel(file_url, index_col='composer')\n",
    "composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnuXfd0GrRpL"
   },
   "source": [
    "### Skipping rows\n",
    "\n",
    "Instead of dropping certains rows at the start or end of the table, we can also directly do that at import time by specifying the ```skipfooter```  and ```skiprows``` arguments. Let's for example remove the first line (note that we remove the row with index 1, i.e. the second one, as we want to keep the headers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1614506327136,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "JVicv0uVr_A-",
    "outputId": "eec7bb45-79ff-47e6-b5be-613608bf3a0f"
   },
   "outputs": [],
   "source": [
    "composers = pd.read_excel(file_url, index_col='composer', skiprows=[1])\n",
    "composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_YZyZausb5r"
   },
   "source": [
    "We can also specify a list of columns that we actually want to use with the ```usecols``` option. For example we could skip the ```city```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1614506327988,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "bfmXQWjgsw2X",
    "outputId": "0fa843f0-0168-411f-d1de-6430d2ceec00"
   },
   "outputs": [],
   "source": [
    "composers = pd.read_excel(file_url, index_col='composer', usecols=['composer','birth', 'death'])\n",
    "composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_afnfGys5Rv"
   },
   "source": [
    "### Format-specific options \n",
    "Finally, for each importer, we have specific options only available in that format. For example with the Excel importer we can specify sheets to import. By default it imports the first one, but we can also indicate the position of the sheet or its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 993,
     "status": "ok",
     "timestamp": 1614506332289,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "vtZwADgbtJhG",
    "outputId": "371dae32-bfd0-4e1a-ec84-a79b8bd5d4bf"
   },
   "outputs": [],
   "source": [
    "composers = pd.read_excel(file_url, sheet_name=4)\n",
    "composers.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96tRltp9EVWs"
   },
   "source": [
    "## Saving tables\n",
    "\n",
    "Most commonly, you don't save many tables. You just keep a notebook or a script for processing and only keep the final output such as a statistics or a plot. However sometimes you want to keep intermediary steps. For that we can use the reverse of the ```read_XXX``` function, i.e the ```to_XXX``` function. Here again, you have plenty of options that you can explore. For example you can avoid saving the index and use ```;``` as separator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1614506453025,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "lzOnjtCnE6RQ"
   },
   "outputs": [],
   "source": [
    "composers.to_csv('export_test.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grv20Sf8KNav"
   },
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "Just for the completeness of this presentation, note that one can also create DataFrames from scratch. There are multiple ways to do this. For example one can turn a simple array into a DataFrame and manually name the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1267,
     "status": "ok",
     "timestamp": 1614506291982,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "cTZOfguLL3Pn",
    "outputId": "590ba508-42d4-4efc-8cd6-8342f4803b4b"
   },
   "outputs": [],
   "source": [
    "my_array = np.random.randint(0, 100, ((3,5)))\n",
    "my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1614506291983,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "HD3BmoYRMD4g",
    "outputId": "4147f038-f550-4793-d6fe-09a731687f87"
   },
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(my_array, columns=['a', 'b', 'c', 'd', 'e'])\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8JCPd89MoZM"
   },
   "source": [
    "Alternatively one can also use dictionaries that allow us to automatically generate appropriate column names. We need here a **dictionary of lists** where each element of the dictionary defines a column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1614506291985,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "cictoB8INfDi"
   },
   "outputs": [],
   "source": [
    "dict_of_list = {\n",
    "    'birth': [1860, 1770, 1858, 1906],\n",
    "    'death':[1911, 1827, 1924, 1975], \n",
    "    'city':['Kaliste', 'Bonn', 'Lucques', 'Saint-Petersburg']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1614506291985,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "0eKb093JNnBa",
    "outputId": "ca59e6b6-4bd6-4c90-a506-90191cff3f61"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_of_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT3AOfgGTHH6"
   },
   "source": [
    "## Exercises\n",
    "1. Import the penguin dataset https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n",
    "2. Rename the ```bill_length_mm``` and ```bill_depth_mm``` colums into ```length``` and ```depth```.\n",
    "3. Check that the names of the DataFrame have changed. If not, do you understand why?\n",
    "4. Add a column with name ```my_column``` and fill it with default value 'test'.\n",
    "\n",
    "5. Import again the data but this time only the 10 first rows. Can you find an option of the ```read_csv``` function to do that?\n",
    "6. Create a new dataframe ```new_dataframe``` by extracting the ```species```, ```bill_length_mm``` and ```body_mass_g``` columns.\n",
    "7. Extract the row with index 4 of ```new_dataframe```\n",
    "8. Extract the ```bill_length_mm``` of the 3 first rows of ```new_dataframe```\n",
    "9. Extract all rows for which the ```body_mass_g > 6000```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMAnw9J8r77GvgnXoJ4g+Rw",
   "collapsed_sections": [],
   "name": "06-Back_to_Pandas.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
