{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a59dc9-bbf7-428d-8d1b-468a396af4da",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Until now we have exclusively looked at **supervised** methods: to create a model we always had a dataset containing **features** and a **target** to predict. The goal in those methods was then to be able to do a **prediction**, i.e. given a set of new samples with possibly unseen features of the same kind, predict the value of another variable (either a *continuous* variable as in regression or a *categorical* variable as in classification). \n",
    "\n",
    "Now we will look at **clustering**, which is an **unsupervised** learning algorithm. In unsupervised learning, we do not have access to any target features. Instead, we try to learn patterns within the features that we do have access to. Samples with similar patterns can then be grouped together within clusters. The idea is that the samples within a cluster should be very similar to each other, and they should be ideally very different from samples of other clusters.\n",
    "\n",
    "Because we do not have targets anymore, the scikit-learn algorithms for unsupervised learning do not expect you to pass a `y` argument anymore. Instead, we only have the features `X` that we pass to the specific classifier during fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aba150-497a-4b2d-87d3-3c0dcc088f1f",
   "metadata": {},
   "source": [
    "## Clustering methods\n",
    "\n",
    "There are many different algorithms that can be used to find clusters, but generally the idea is to find sub-groups in our dataset where data points are close together according to some metric. We will fist look at some artificial data to get better understand clustering. For this we use a scikit-learn function that creates blobs of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05020310-6a4f-44a6-89c3-b1ce69792d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c273b0b-06c9-4b1d-b5a1-d3d5b965b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, labels = make_blobs(n_samples=30, n_features=2, centers=3, random_state=42)\n",
    "\n",
    "blobs = pd.DataFrame(blobs, columns=['feature1', 'feature2'])\n",
    "blobs['label'] = labels\n",
    "\n",
    "blobs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351dc4f-6a79-4fc9-aaba-69a0156e9bb1",
   "metadata": {},
   "source": [
    "Above, we generated a DataFrame containing two features and label. The label tells us, to which cluster a sample belongs to. This is just for demonstration pruposes, of course we would not have access to such information in advance. The clustering algorithm would then find this cluster membership mapping for each sample in our dataset. Let us plot these artificial samples in a scatterplot as they are 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b3186-14b1-4f9b-8915-e7ef02724fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca65779-bdba-4bc9-8e19-e2b2992ee211",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=blobs, x='feature1', y='feature2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf045bf-9456-4bd6-b069-fe4eefa6ab34",
   "metadata": {},
   "source": [
    "## *k*-Means Clustering\n",
    "\n",
    "In this toy example, we can cleary see that there are three different groups of samples. Now let us try to discover these three clusters using a clustering algorithm. We will first look at *k*-Means, which is a rather simple clustering algorithm. \n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "#### 1. Decide in advance how many clusters you expect in the dataset. This is what the *k* in *k*-Means refers to. In our case, we set $k=3$.\n",
    "\n",
    "#### 2. For each of these *k* clusters, we randomly initialize a cluster center. In this example, we choose the initial cluster centers manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26b833-6a3a-4b74-8e34-cfc03b92ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.array([[-1, -10], [-4, 0], [-1.5,-2]])\n",
    "\n",
    "sns.scatterplot(data=blobs, x='feature1', y='feature2')\n",
    "plt.plot(cluster_centers[:,0], cluster_centers[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0a29c-1a23-4143-85b1-2cb6f2cd2ab0",
   "metadata": {},
   "source": [
    "#### 3. For each sample in our dataset (green points), we compute the distance to the cluster centers (red diamonds) and assign each sample to closest cluster.\n",
    "\n",
    "\n",
    "Below you can see how you can manually compute this distance from each sample to all of the three cluster centers in our toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fcf22-09ce-403b-b4eb-a30a0605cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for center in cluster_centers:\n",
    "    # Calculate the distance from each point to this specific cluster center\n",
    "    distance = np.linalg.norm(blobs[['feature1', 'feature2']].values - center, axis=1)\n",
    "    distances.append(distance)\n",
    "\n",
    "# Convert the list of distances (list of numpy arrays) to a 2D numpy array\n",
    "distances = np.stack(distances, axis=1)\n",
    "\n",
    "# Show the array, each row corresponds to a point, and each column corresponds to the distance to a cluster center\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e45ddd-e19f-4c03-8dad-c6c440f35be1",
   "metadata": {},
   "source": [
    "You can think of the clusters as being labeles with an id between $0$ and $k-1$. For each sample in our dataset, we want to find the cluster id of the cluster center that is closest to that sample. We could apply the `np.min` function across the columns to find the minimum distance in each row. However, this would not give use the id of the cluster but only the distance. Fortunately, there is another function that we can used: `np.argmin`. The function does one extra step: it first searches for the minimum, but then it does not return the minimum but rather the index at which the minimum was found. We use that function below to get the cluster assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c3493-bca9-4f5b-9721-5a7095ea1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = np.argmin(distances, axis=1)  # axis = 1 because we want to find the minimum distance across the columns\n",
    "min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b9f6e-1e3a-4be0-bf50-10a02cc50e79",
   "metadata": {},
   "source": [
    "Now we can assign these cluster labels to our DataFrame as a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45527e21-8137-4cd3-ab52-3249b3426437",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs['cluster_label'] = min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac963948-92c8-4e99-9d76-e94ef53660a8",
   "metadata": {},
   "source": [
    "Below, we plot the actual labels that we got from the `make_blobs` function on the left, and the labels that we just inferred from the distances to the cluster centers on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8147b-9756-4a01-b52c-df4d195973f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.scatterplot(data=blobs, x='feature1', y='feature2', hue='label', palette='Set2', ax=ax[0])\n",
    "sns.scatterplot(data=blobs, x='feature1', y='feature2', hue='cluster_label', palette='Set2', ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16097c5-a5d0-4f0e-9a91-270fdff2792a",
   "metadata": {},
   "source": [
    "After the first iteration, we can see that in the cluster on the lower left there are some point that are assigned the wrong cluster. Let us move on with the second iteration.\n",
    "\n",
    "#### 4. Now we re-compute the cluster centers. To do so, we select the samples that are assigned to a specific cluster, and then we compute the average feature values. This will give us a new cluster center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ff66e-3115-4fff-add4-c13a66034509",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = []\n",
    "for label in blobs.cluster_label.unique():\n",
    "    # Calculate the mean of the points in this cluster\n",
    "    center = blobs[blobs.cluster_label == label][['feature1', 'feature2']].mean().values\n",
    "    cluster_centers.append(center)\n",
    "\n",
    "\n",
    "cluster_centers = np.stack(cluster_centers, axis=0)  # axis = 0 because we want to stack along the first dimension (rows)\n",
    "\n",
    "print(f\"Shape of cluster_centers (num_samples, num_features): {cluster_centers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09f119-6294-40a4-916b-9c3c57f6aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=blobs, x='feature1', y='feature2')\n",
    "plt.plot(cluster_centers[:,0], cluster_centers[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800bfb5-d1e5-480d-aac1-a0f853ee0119",
   "metadata": {},
   "source": [
    "Because we still had some samples that were not assigned to their actual cluster, the new cluster centers that we computed are not entirely centered yet. But if we keep on performing the same steps as before, this will change.\n",
    "\n",
    "#### 5. We can repeat the same operation as before: we assign points to the closest cluster center and update the position of the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd047078-9088-4278-82d5-e522fcda4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do the same as before, but this time in the form of a list comprehensions\n",
    "distances = np.stack([np.linalg.norm(blobs[['feature1', 'feature2']].values - center, axis=1) for center in cluster_centers], axis=1)\n",
    "min_dist = np.argmin(distances, axis=1)\n",
    "blobs['cluster_label'] = min_dist\n",
    "cluster_centers = np.stack([blobs[blobs.cluster_label == label][['feature1', 'feature2']].mean().values for label in blobs.cluster_label.unique()], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.scatterplot(data=blobs, x='feature1', y='feature2', hue='label', palette='Set2', ax=ax[0])\n",
    "sns.scatterplot(data=blobs, x='feature1', y='feature2', hue='cluster_label', palette='Set2', ax=ax[1])\n",
    "plt.plot(cluster_centers[:,0], cluster_centers[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c44f6-edfa-45ba-8967-a87ac3ed8fe4",
   "metadata": {},
   "source": [
    "We successfuly detected the clusters already in the second iteration! In practice, the algorithms needs usually more iterations, as the data is not as clearly separated as in this toy example. You can imagine that some things can go wrong. For example, if the initial (random) cluster centers are poorly initialized, two cluster centers might converge to the same location. However, many libraries try to avoid that by choosing good initial cluster centers that are spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50b80c-0548-4a86-8eea-dc75f2de2e6a",
   "metadata": {},
   "source": [
    "## Application in scikit-learn\n",
    "\n",
    "Now that you understand how the algorithm works, let us have a look at how we can use it out-of-the-box with scikit-learn. We start by importing `KMeans` from the `sklearn.cluster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41e4bc-1193-47a6-9274-5e34067af949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ceee5b-4ff0-4481-8d9b-baa9e1107f8a",
   "metadata": {},
   "source": [
    "Then we instantiate our model with the required parameters. We need to specify the number of cluster that we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922335b-1468-43db-b1fb-3f1b66b59654",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfa459-9346-4a5a-b119-94b741fb3cb3",
   "metadata": {},
   "source": [
    "Then we fit our model. Since this is an unsupervised learning algorithm and we do not have a targets, we only need to specify the features ```X```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b67546-54f7-47f5-99b3-512826fa3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = blobs[['feature1', 'feature2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d386e-9e30-499f-942c-cd7d0cc762b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02607167-9891-49bd-966b-7998c683bcae",
   "metadata": {},
   "source": [
    "The cluster centers that the algorithm found are stored under the `cluster_centers_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f4ac49-e4bd-4d45-89e3-6b9758cd9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b8a7c-0bbb-4a34-92e8-3f389f457696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the cluster assignments for each sample in our dataset\n",
    "kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1fe640-3425-4545-8c89-3838119af548",
   "metadata": {},
   "source": [
    "Let us see if they correspond to our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d9b44-e6dc-43a9-9901-c6908ad27c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=blobs, x='feature1', y='feature2', hue=kmeans_model.labels_, palette='Set2', ax=ax)\n",
    "plt.plot(kmeans_model.cluster_centers_[:,0], kmeans_model.cluster_centers_[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf93594-2521-4716-9f81-6ec123e8a809",
   "metadata": {},
   "source": [
    "### Comparing with Real Labels\n",
    "\n",
    "In practice, clustering is used without labels. But when testing on labeled data, we can compare the results to the true classes.\n",
    "\n",
    "Keep in mind: cluster labels are arbitrary — k-Means might label the \"green\" group as 2 instead of 0. So even with perfect clustering, the label numbers may not match.\n",
    "\n",
    "To fairly evaluate, you can:\n",
    "- Match clusters to true labels using majority vote, or  \n",
    "- Use metrics like Adjusted Rand Index that ignore label order.\n",
    "\n",
    "This gets harder when clusters overlap or contain outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e6a01-be19-4809-a967-55278908139e",
   "metadata": {},
   "source": [
    "## Real case\n",
    "\n",
    "Let us now look at a real dataset. The following table contains information about the size (Lb - \"cell **l**ength at **b**irth\") and growth rate of bacteria growing in two different conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089e62c-4105-4113-9b3b-3ee2a75d17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria = pd.read_csv('https://raw.githubusercontent.com/digital-sustainability/SAI3-2025/refs/heads/main/datasets/coli.csv')\n",
    "bacteria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036056b-f2ea-4100-9d4f-70318d5316c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue='condition', palette='Set2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99f14f-df39-4b9c-81e9-c8864145e3c9",
   "metadata": {},
   "source": [
    "Again, we happen to know the two conditions/labels in this example. But you might imagine an experiment where you have different populations of cells measured at the same time and wish to identify different groups. Let us initialize a clustering model with two clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e88d1-6d57-4001-bc80-de12f60e4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bacteria[['Lb', 'growth_rate']]\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=2)\n",
    "kmeans_model.fit(X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38be268-d489-4bf6-8dcb-4c166f966ec2",
   "metadata": {},
   "source": [
    "Now we will check the predicted cluster assignments and compare it to the actual conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2302f8-51f6-4875-a6fd-18d9b96cf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue='condition', palette='Set2', ax=ax[0])\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue=kmeans_model.labels_, palette='Set2', ax=ax[1])\n",
    "plt.plot(kmeans_model.cluster_centers_[:,0], kmeans_model.cluster_centers_[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf22184-2c4a-44c1-ab69-c7e277e5a349",
   "metadata": {},
   "source": [
    "The algorithm identified two clusters, but they do not match the actual conditions or what we would expect from the scatterplot. This mismatch is due to a familiar issue: the two features are on very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d35e5-3fd5-47c6-8c25-b8e01aede2f9",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "In the case of K-Means clustering the reason is particularly evident: as we measure distances between points, if one feature has a much larger scale, then it will dominate the clustering, i.e. the data will mainly be partitioned along one given axis as is the case here with the length ```Lb```. We can now check whether scaling our feature in the above case, could help obtain a better clustering. Again we use the ```preprocessing``` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071b493-f5fe-4765-8f72-550939e45935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cbec6-b133-4176-a926-9a7f481753ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_scaled = std_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5bf08-a7cd-42c7-a6b4-0db27dd78eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=2)\n",
    "kmeans_model.fit(X=X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020370c-688e-4e64-b956-ffd0eec89a16",
   "metadata": {},
   "source": [
    "Note that when we want to plot things in the original (unscaled) space, we need to *reverse* the scaling. You can use the `inverse_transform` function for this. Below is an example of the scaled cluster centers and the unscaled cluster centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a097a49-bb78-4c1b-9f19-fa02a58c7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler.inverse_transform(kmeans_model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a74bc4-4ae0-4a80-bf6e-403dce2fd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue='condition', palette='Set2', ax=ax[0])\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue=kmeans_model.labels_, palette='Set2', ax=ax[1])\n",
    "plt.plot(std_scaler.inverse_transform(kmeans_model.cluster_centers_)[:,0], std_scaler.inverse_transform(kmeans_model.cluster_centers_)[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc98e2a-1362-4d55-8230-9d986876c5f8",
   "metadata": {},
   "source": [
    "We see that the scaling fixes the problem! Certain points are still in the wrong cluster but that will always be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0fcb3-1c51-48e1-99ba-660d1890fef9",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "As mentioned above, there are many ways to detect clusters in a dataset. Just for the purpose of illustration, we show here two alternatives called Mean Shift and DBScan clustering which are capable of finding clusters in smooth distributions and can determine the number of clusters on their own, i.e. we don't have to provide a ```n_clusters``` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d421bf-b5d1-40e3-97a4-695b4f4044da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7cabb-8f30-4d55-a305-595d68e31ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model = MeanShift()\n",
    "ms_model.fit(X=X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce9d23-6f1d-4fdf-bb83-8139ef16b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue='condition', palette='Set2', ax=ax[0])\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue=ms_model.labels_, palette='Set2', ax=ax[1])\n",
    "plt.plot(std_scaler.inverse_transform(ms_model.cluster_centers_)[:,0], std_scaler.inverse_transform(ms_model.cluster_centers_)[:,1], 'rD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482f66e-c9d9-48ca-8411-69cf3085bad9",
   "metadata": {},
   "source": [
    "The algorithm performs well overall but creates extra clusters for isolated points. These outliers can be removed in a post-processing step.\n",
    "\n",
    "Next, we try DBSCAN, a method that identifies dense regions and expands clusters from them. A key parameter is `eps`, which defines the maximum distance between points in the same cluster. You can experiment with different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58f523-c191-412e-b3fd-0a4ab0da9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model = DBSCAN(eps=0.15)\n",
    "db_model.fit(X=X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e2e50-04b3-4d0a-9292-e609bd2a1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue='condition', palette='Set2', ax=ax[0])\n",
    "sns.scatterplot(data=bacteria, x='Lb', y='growth_rate', hue=db_model.labels_, palette='Set2', ax=ax[1])\n",
    "ax[1].legend(ncols=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bc3a4-2c67-4bef-a86a-501b0c83473b",
   "metadata": {},
   "source": [
    "We see that the method is also capable of finding relevant clusters but with a fairly different results from previous solutions. It is difficult to tell in advance which clustering methods and hyperparameters are optimal for a problem, and often some trial and error is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81e731-b4e3-4a40-8887-0cd96b35c3a0",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Import the ```movement.csv``` dataset\n",
    "2. Visualize as scatter plot of ```z_acc``` and ```y_acc``` and color by the ```move_type```.\n",
    "3. Try to cluster the data with these features. Try KMeans and DBSCAN. Do you achieve a good clustering? Try out different `eps` values.\n",
    "\n",
    "### Bonus\n",
    "4. Use the following code snippet to load the moons dataset:\n",
    "   \n",
    "   ```python\n",
    "    from sklearn.datasets import make_moons\n",
    "    moons, labels = make_moons(n_samples=300, noise=0.1)\n",
    "   ```\n",
    "5. Cluster the moons dataset with DBSCAN and visualize it with two scatter plots in the same row: on the left with the true labels returned by `make_moons`, on the right with the learned labels.\n",
    "\n",
    "    - Try varying `eps` and `min_samples` and see what happens\n",
    "    - What happens when you use more noise?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
