{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d85ff19-6aeb-4ae6-bd2a-d5d2e9939788",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In previous chapters, we explored classifiers that rely on mathematical optimization. In this notebook, we'll take a different route and introduce a model that mimics human reasoning: **decision trees**.\n",
    "\n",
    "Rather than relying on numerical optimization, decision trees build a sequence of rules—such as *\"If an apple is red and large, then it is good\"*—to classify data points. This logic-based approach is intuitive and can be visualized as a flow of decisions, which makes decision trees particularly useful for both learning and interpreting classification tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b8d3e5-a225-4ac7-aa36-4857cd6ddabb",
   "metadata": {},
   "source": [
    "## 1.1 Toy Example\n",
    "\n",
    "To understand decision trees intuitively, let's begin with a toy dataset. Imagine we want to classify apples as **good** or **bad** based on three binary features:\n",
    "\n",
    "- **Size**: large or small  \n",
    "- **Color**: red or green  \n",
    "- **Spots**: present or absent\n",
    "\n",
    "Each apple also comes with a label: whether it is good or bad. Our goal is to discover decision rules such as:\n",
    "\n",
    "> “If an apple is large and green, then it is good.”\n",
    "\n",
    "This is precisely the kind of logic that decision trees aim to formalize: a sequence of feature-based rules that result in a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892aebbe-57b1-448a-a22d-7cd0cbeca54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../illustrations/apple_tree.png', width=900)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f31a6b2c-c32f-44cb-9098-58ae04948c35",
   "metadata": {},
   "source": [
    "In this image, each apple is described by its features and labeled as either *Good* or *Bad*. Given this, we can try to identify patterns:\n",
    "\n",
    "- Are large apples more likely to be good?\n",
    "- Does color play a role?\n",
    "- Do spots indicate bad quality?\n",
    "\n",
    "While this is manageable by eye for a small dataset, things get complex when we have many features or thousands of samples. This is where decision trees shine: they help us **automate the discovery of such rules**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2087f02-0207-4177-9d6d-95b87b6cb0f4",
   "metadata": {},
   "source": [
    "## 1.2 How to Formalize Rules\n",
    "\n",
    "As humans, we often make hypotheses based on a few examples and test whether these rules generalize. However, when dealing with large datasets and many features, we need a **systematic** way to generate and evaluate these rules.\n",
    "\n",
    "This is exactly what decision trees do. Their core logic follows a simple procedure:\n",
    "\n",
    "1. **Test multiple feature-based questions** to split the data.\n",
    "2. **Select the most informative split**.\n",
    "3. **Repeat the process** recursively on each subset until all items are classified or a stopping criterion is met.\n",
    "\n",
    "Let’s explore how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96693d",
   "metadata": {},
   "source": [
    "### 1.2.1 Creating Splits\n",
    "\n",
    "For any feature, we can ask a **yes/no question** to split our dataset into two groups. For example:\n",
    "\n",
    "- “Is the apple red?”\n",
    "- “Is the apple large?”\n",
    "\n",
    "Each question creates a **binary split**, helping us organize the data into smaller, hopefully more homogeneous groups. \n",
    "\n",
    "If we ask *\"Is the apple red?\"*, we get the following two groups for the answers no and yes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1568ac-0468-491c-9c32-6ace87851eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../illustrations/apple_tree_split.png', width=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f4cead7-6188-492f-9e35-f691e7448079",
   "metadata": {},
   "source": [
    "Alternatively, if we ask *\"is the apple large?\"*, we obtain the following two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3014c37-eff9-4116-b520-47b718dba28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../illustrations/apple_tree_split2.png', width=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebee0b80-e664-4b6f-a151-30847a96acaa",
   "metadata": {},
   "source": [
    "We can see that neither of these simple questions perfectly separates the apples into good and bad categories—both splits still contain a mix of labels.\n",
    "\n",
    "To build an effective decision tree, we need to **evaluate all possible splits** and choose the one that results in the **most distinct separation** between labels.\n",
    "\n",
    "So, how do we quantify the \"best\" split? One common approach is to measure how *pure* each resulting group is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31fe7c3c-6725-4639-b618-1dfe8cfc4194",
   "metadata": {},
   "source": [
    "### 1.2.2 Best Splits and Entropy\n",
    "\n",
    "A good split separates the data into groups that are as **pure** as possible—that is, each group contains items of mostly one label. One way to measure this purity is using a concept from information theory called **entropy**.\n",
    "\n",
    "Entropy quantifies how mixed a subset is. In physics, entropy measures disorder; in classification, it tells us how uncertain or impure a group is.\n",
    "\n",
    "For a binary classification task, entropy is defined as:\n",
    "\n",
    "$$\n",
    "E = -p_0 \\log(p_0) - p_1 \\log(p_1)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p_0$ is the fraction of one class (e.g., \"bad\" apples),\n",
    "- $p_1 = 1 - p_0$ is the fraction of the other class (e.g., \"good\" apples).\n",
    "\n",
    "Entropy reaches its maximum when both classes are equally mixed ($p_0 = 0.5$), and it is zero when all elements belong to the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f680c3-bc43-4d07-9a4a-2047286a9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def entropy(x):\n",
    "    return -x * np.log(x) - (1 - x) * np.log(1 - x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x_vals = np.arange(0.01, 1, 0.01)\n",
    "ax.plot(x_vals, entropy(x_vals), 'k')\n",
    "ax.set_xlabel(r\"$p_{\\text{red}}$\", fontsize=16)\n",
    "ax.set_ylabel(\"Entropy\", fontsize=16)\n",
    "ax.set_ylim(0.1, 1.1)\n",
    "ax.set_xlim(-0.8, 1.8)\n",
    "ax.set_xticks([0, 0.5, 1])\n",
    "\n",
    "positions = [\"lower left\", \"upper center\", \"lower right\"]\n",
    "for i, pos in enumerate(positions):\n",
    "    im = PILImage.open(f\"../illustrations/entropy{i+1}.png\")\n",
    "    axins = inset_axes(ax, width=\"30%\", height=\"40%\", loc=pos)\n",
    "    axins.imshow(im)\n",
    "    axins.set_axis_off()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d956aa60-d811-4ec2-9d86-ae365d27c03a",
   "metadata": {},
   "source": [
    "As shown in the plot above, entropy is highest when the dataset is perfectly mixed (e.g., 50% red, 50% blue), and lowest when the dataset is pure (e.g., all red or all blue).\n",
    "\n",
    "This gives us a simple rule:  \n",
    "> **The lower the entropy after a split, the better the split.**\n",
    "\n",
    "In practice, we compute the entropy for every possible feature-based split and choose the one that produces the **purest** subsets — i.e., the lowest combined entropy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6bf134-0800-4821-9bc2-af9c09dda795",
   "metadata": {},
   "source": [
    "### 1.2.3 Information Gain\n",
    "\n",
    "While entropy measures how mixed a group is, what we actually care about is how much **better** a split makes our classification.\n",
    "\n",
    "This is captured by **information gain (IG)**, which tells us how much entropy is **reduced** by splitting the data using a specific feature.\n",
    "\n",
    "The formula for information gain is:\n",
    "\n",
    "$$\n",
    "IG = E_{\\text{parent}} - \\left( \\frac{n_1}{N} E_1 + \\frac{n_2}{N} E_2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $E_{\\text{parent}}$ is the entropy before the split,\n",
    "- $E_1$, $E_2$ are the entropies of the two resulting subsets,\n",
    "- $n_1$, $n_2$ are their respective sizes, and\n",
    "- $N = n_1 + n_2$ is the size of the parent set.\n",
    "\n",
    "This formula gives higher scores to splits that:\n",
    "- Greatly reduce entropy, and\n",
    "- Affect a large portion of the data.\n",
    "\n",
    "Splits that isolate a single point (very small $n_1$ or $n_2$) typically have **low** information gain and are therefore avoided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9d261c6-0ba2-4c0a-96af-34ef8ee3e7d6",
   "metadata": {},
   "source": [
    "## 1.3 Splitting Apples\n",
    "\n",
    "Let’s now apply the above ideas to our apple dataset.\n",
    "\n",
    "Suppose we compute the information gain for all features on the initial dataset. We find that the most informative one is **color**. This leads to the first split:\n",
    "\n",
    "> \"Is the apple green?\"\n",
    "\n",
    "This split separates the apples into two groups. From there, we repeat the process recursively on each group, always choosing the next most informative feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74944486-3f61-41a1-ae5e-0cd111fd8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../illustrations/apple_first_split.png', width=600)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d985b59-5a0b-4434-a3cb-321ce117aacd",
   "metadata": {},
   "source": [
    "## 1.4 Recursive Splitting\n",
    "\n",
    "After the first split, we now treat each resulting subset as a smaller classification problem.\n",
    "\n",
    "For the **left subset** (e.g., green apples), the most informative feature might be **size**:\n",
    "> \"Is the apple large?\"\n",
    "\n",
    "This results in perfectly pure groups: one contains only good apples, the other only bad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee675f-91d3-4d36-949a-8c3a8d23801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../illustrations/apple_second_split_1.png', width=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "721159b1-d37a-41d5-bc7d-9499ef39ad84",
   "metadata": {},
   "source": [
    "For the **right subset** (e.g., red apples), the best feature might be **spots**:\n",
    "> \"Does the apple have spots?\"\n",
    "\n",
    "Again, this creates completely pure groups.\n",
    "\n",
    "At this point, all examples are perfectly classified — and the tree is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed76a5-60f6-4110-bf87-d10a75d0ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../illustrations/apple_second_split_2.png', width=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff8bdf64-9234-4daa-ae28-5e7e2c19d41a",
   "metadata": {},
   "source": [
    "## 1.5 The Final Tree\n",
    "\n",
    "After applying all the splits, we can represent the entire classification process as a **decision tree**.\n",
    "\n",
    "Each internal node corresponds to a question about a feature.  \n",
    "Each leaf node gives us a classification result (e.g., good or bad apple).\n",
    "\n",
    "This tree can now be used to classify **new** apples: we simply follow the branches based on their features until we reach a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001d4aa-fb07-41f3-8aa9-b003eec3c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../illustrations/apple_decision_tree.png', width=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e0c263-f0cf-4504-a200-9c791606496e",
   "metadata": {},
   "source": [
    "### 1.5.1 Non-Categorical Variables\n",
    "\n",
    "In our example, we only used **categorical** features (e.g., size = large/small, color = red/green).  \n",
    "However, decision trees can easily handle **continuous** variables as well.\n",
    "\n",
    "Instead of splitting based on discrete categories, the tree chooses a **threshold**:\n",
    "\n",
    "> “Is the apple’s size greater than 8 cm?”\n",
    "\n",
    "This kind of question also creates two groups, and the tree selects the **best threshold** by maximizing the information gain — just as before.\n",
    "\n",
    "This flexibility makes decision trees suitable for both numerical and categorical data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a09536f-4c08-452a-b7ac-ba9d029f2374",
   "metadata": {},
   "source": [
    "## 1.6 Decision Trees in scikit-learn\n",
    "\n",
    "Let’s now build a real decision tree using `scikit-learn`.\n",
    "\n",
    "We’ll work with the **seeds dataset** from the UCI repository, which contains several measurements of wheat kernels. Each row describes a seed using features like area, perimeter, length, and groove length, and includes a label indicating the seed type.\n",
    "\n",
    "We’ll begin by importing the relevant class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebcabf6-2d16-4fa1-97fe-dc7a95f8097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from UCI repository\n",
    "seeds = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt',\n",
    "    sep='\\t',\n",
    "    on_bad_lines='skip',\n",
    "    names=['area', 'perimeter', 'compactness', 'length', 'width',\n",
    "           'symmetry_coef', 'length_groove', 'seed_type']\n",
    ")\n",
    "\n",
    "seeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ce459-d03a-4b83-9789-e706851ebd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrix X and target vector y\n",
    "X = seeds[['area', 'perimeter', 'compactness', 'length', 'width',\n",
    "           'symmetry_coef', 'length_groove']]\n",
    "y = seeds['seed_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51347b24-c987-402a-9a29-5fefbe8fc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier using entropy as the splitting criterion\n",
    "tree_clf = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# Train the model on the data\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb258d-990c-4b6d-938f-9d15762a167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the trained decision tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(tree_clf, filled=True, ax=ax, feature_names=X.columns)\n",
    "plt.title(\"Decision Tree for Seeds Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b741f10d-11ad-4d88-ad60-6601c7af17b6",
   "metadata": {},
   "source": [
    "Each node in the tree corresponds to a question like:\n",
    "\n",
    "> “Is groove length less than 5.5?”\n",
    "\n",
    "The dataset is recursively split based on these questions. Each node displays:\n",
    "- The condition used to split\n",
    "- The entropy value\n",
    "- The number of samples\n",
    "- The sample distribution across classes/categories\n",
    "\n",
    "The **leaf nodes** (bottom boxes) represent final predictions. Their entropy is often 0, indicating pure subsets.\n",
    "\n",
    "The **colors** reflect the predicted class, and the color intensity shows how pure the node is.  \n",
    "We can also observe that some features, like `length_groove`, are used multiple times—showing their importance in classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9553c2f-d45d-4d66-b221-463a2ce414c3",
   "metadata": {},
   "source": [
    "### 1.6.1 Adjusting Tree Parameters\n",
    "\n",
    "In our earlier tree, some leaves contain only a single sample. This indicates **overfitting**: the model tries too hard to perfectly fit the training data.\n",
    "\n",
    "To avoid overfitting, we can control the tree’s complexity by setting parameters such as:\n",
    "\n",
    "- `max_depth`: limits the number of decision levels in the tree\n",
    "- `min_samples_leaf`: sets the minimum number of samples per leaf node\n",
    "\n",
    "Let’s see what happens when we limit the tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dec757-d47e-4a4e-b423-b50ed672eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the depth of the tree to prevent overfitting\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy')\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "# Plot the simplified tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(tree_clf, filled=True, ax=ax, feature_names=X.columns)\n",
    "plt.title(\"Decision Tree with max_depth=2\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9c6b422-b097-4973-9b29-7976b5ea0d9a",
   "metadata": {},
   "source": [
    "By limiting the tree depth to 2, we now have a much **simpler model**.\n",
    "\n",
    "- The tree is easier to interpret.\n",
    "- It generalizes better to unseen data.\n",
    "- However, it may not classify the training data as accurately as a deeper tree.\n",
    "\n",
    "We can also see that the **leaf nodes now have non-zero entropy**, meaning the splits are no longer perfectly pure — and that's okay! Simpler trees often perform better on new data.\n",
    "\n",
    "Alternatively we can also specify a minimum number of elements per split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75050047-f866-4faf-9845-b6c4b945f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require at least 10 samples per leaf to reduce overfitting\n",
    "tree_clf = DecisionTreeClassifier(min_samples_leaf=10, criterion='entropy')\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "# Plot the resulting tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(tree_clf, filled=True, ax=ax, feature_names=X.columns)\n",
    "plt.title(\"Decision Tree with min_samples_leaf=10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f3efe",
   "metadata": {},
   "source": [
    "Setting `min_samples_leaf=10` means that each final group must contain **at least 10 samples**.\n",
    "\n",
    "This prevents the tree from splitting on **tiny patterns** that may not generalize well, and helps reduce overfitting. As a result:\n",
    "\n",
    "- Some potentially pure splits are avoided.\n",
    "- The model trades off a bit of accuracy for better generalization.\n",
    "\n",
    "Just like with `max_depth`, this is part of **regularization**—controlling model complexity to balance training accuracy and test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de92b95",
   "metadata": {},
   "source": [
    "## 1.7 Random Forests\n",
    "\n",
    "A **random forest** is an ensemble method that builds not one, but many decision trees, and then combines their predictions.\n",
    "\n",
    "Instead of relying on a single tree (which might overfit), random forests:\n",
    "- Train multiple trees on **random subsets** of the data (with replacement),\n",
    "- Use **random subsets of features** for each split,\n",
    "- Aggregate the predictions (e.g., by majority vote).\n",
    "\n",
    "This makes them more robust, often achieving better accuracy and generalization. One advantage of this method is that it can find interesting patterns existing in subsets of the data, as the model doesn't have to satisfy all constraints at the same time.\n",
    "\n",
    "Let’s try it out in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ec708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier with 50 trees\n",
    "rf_model = RandomForestClassifier(n_estimators=50)\n",
    "rf_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50a74a",
   "metadata": {},
   "source": [
    "Since we now learned 50 different trees, we cannot visualize them anymore. However, Random Forests can give us insights into the importance of each feature in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importances\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_  # Stores information about feature importance\n",
    "}).sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6c91c",
   "metadata": {},
   "source": [
    "The table above shows how much each feature contributed to the forest's decision-making process.\n",
    "\n",
    "- Higher importance values mean that the feature was used more often, and more effectively, to split the data.\n",
    "- In this example, we can see that `length_groove` is the most informative feature—just like in the single decision tree!\n",
    "\n",
    "Unlike a single tree, a random forest doesn’t produce an interpretable decision path, but it often results in **higher accuracy** and **better generalization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb14a0d",
   "metadata": {},
   "source": [
    "## Exercise: Predict House Quality with a Decision Tree\n",
    "\n",
    "Let’s now apply what we’ve learned on a real-world dataset!\n",
    "\n",
    "Use the **King County housing dataset** `kc_house_data.csv`, which includes house features and sale prices.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. Import the dataset\n",
    "\n",
    "2. Create a new binary column `good_bad`:  Set it to 1 if `grade > 7`, else 0.\n",
    "\n",
    "3. Remove outliers:  Keep only rows where `sqft_living < 8000` and `bedrooms < 10`.\n",
    "\n",
    "4. Split the dataset into training and test sets  (e.g., 80% training, 20% testing)\n",
    "\n",
    "5. Train a `DecisionTreeClassifier` using these features: `price`, `bedrooms`, `bathrooms`, `sqft_living`, `sqft_lot`, `floors`. Use entropy as the criterion and a max depth of three.\n",
    "\n",
    "6. What is the most informative feature (top of the tree)?\n",
    "\n",
    "7. Evaluate model accuracy.\n",
    "\n",
    "8. Increase `max_depth` to 25, retrain, and re-evaluate. Does accuracy improve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
