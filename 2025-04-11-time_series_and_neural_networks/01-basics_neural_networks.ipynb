{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708e6745",
   "metadata": {},
   "source": [
    "# 1. Introduction to Neural Networks via Regression\n",
    "\n",
    "In the previous chapter, we introduced **linear regression**, where we found a model that describes a target variable $y$ using a set of features $x_i$. In its simplest form, a linear model is defined as:\n",
    "\n",
    "$$\n",
    "y = w_0 x_0 + w_1 x_1 + \\dots + b\n",
    "$$\n",
    "\n",
    "We also learned how to optimize the parameters $w_i$ and $b$ by minimizing the prediction error using **gradient descent**, which adjusts parameters in the direction of the steepest descent of the loss function.\n",
    "\n",
    "In this chapter, we will reformulate this problem using a **neural network**. While neural networks are widely used in deep learning and can become quite complex, their fundamental structure is relatively simple. Linear regression serves as an excellent entry point to understand how neural networks work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b60bc8",
   "metadata": {},
   "source": [
    "## 1.1 What Is a Neural Network?\n",
    "\n",
    "Neural networks are inspired by early models of the brain. Biological neurons are specialized cells connected to each other through structures called axons, forming vast networks of interconnected cells. These connections can strengthen with experience, enabling learning.\n",
    "\n",
    "In computational models, we simplify this concept. A **neural network** consists of artificial neurons (nodes) connected by weighted links. Each connection has a **weight** that determines its strength. The basic building block of a neural network resembles the following:\n",
    "\n",
    "$$\n",
    "y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "This is the same equation we used in linear regression.\n",
    "\n",
    "Below, we show a an illustration of two connected neurons (left) and a network of cortical neurons of a mouse (right; source: <a href=\"https://commons.wikimedia.org/wiki/File:Neuronal_web.tif\">ALol88</a>, <a href=\"https://creativecommons.org/licenses/by/4.0\">CC BY 4.0</a>, via Wikimedia Commons):\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../illustrations/two_connected_neurons.png\" style=\"max-width: 95%; height: auto;\"/>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "        <img src=\"../illustrations/neurons.jpg\" style=\"max-width: 95%; height: auto;\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d8996",
   "metadata": {},
   "source": [
    "Neurons communicate through electrical signals, and the strength of their connections can change with experience. If two neurons are frequently activated together, their connection tends to become stronger. These biological networks are vast and highly interconnected.\n",
    "\n",
    "Artificial neural networks mimic this idea. They consist of:\n",
    "\n",
    "- **Nodes (neurons)** that represent inputs or internal activations,\n",
    "- **Connections (weights)** that determine how strongly one neuron influences another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"../illustrations/networks1.png\", width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f78e",
   "metadata": {},
   "source": [
    "The neural network above is the simplest possible configuration. It directly corresponds to the linear regression model introduced earlier:\n",
    "\n",
    "$$\n",
    "y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "In this representation:\n",
    "\n",
    "- $x_0, x_1, x_2$ are the input features,\n",
    "- $w_0, w_1, w_2$ are the weights (or parameters),\n",
    "- $b$ is the bias (intercept),\n",
    "- $y$ is the predicted output.\n",
    "\n",
    "This example shows that a basic neural network can implement a linear model. As we will see, we can build more powerful models by adding layers and introducing non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4a0a1",
   "metadata": {},
   "source": [
    "## 1.2 Tensors\n",
    "\n",
    "To implement a neural network in code, we need a data structure that can represent and manipulate multi-dimensional arrays. While we could use NumPy arrays for basic operations, deep learning frameworks rely on a more flexible structure called a **tensor**.\n",
    "\n",
    "Tensors are similar to NumPy arrays but come with additional features:\n",
    "\n",
    "1. They support computation on **Graphical Processing Units (GPUs)** for faster training.\n",
    "2. They integrate with **automatic differentiation**, which is essential for gradient-based optimization.\n",
    "\n",
    "In this notebook, we use PyTorch, which provides a `torch.Tensor` object with NumPy-like syntax and deep learning capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa626e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 3x4 tensor filled with zeros\n",
    "torch_array = torch.zeros((3, 4))\n",
    "torch_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a4194",
   "metadata": {},
   "source": [
    "We created a $3 \\times 4$ tensor filled with zeros. This behaves very similarly to a NumPy array.\n",
    "\n",
    "Many functions in NumPy have equivalents in PyTorch. For example, `torch.zeros` is analogous to `np.zeros`. The key difference is that PyTorch tensors can be used in deep learning pipelines, optimized on GPUs, and integrated into automatic differentiation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ecb66f",
   "metadata": {},
   "source": [
    "## 1.3 Layers\n",
    "\n",
    "In the neural network diagram shown earlier, the values $x_0, x_1, x_2$ form the **input layer**. A layer is simply a group of neurons (or units) that process inputs in parallel.\n",
    "\n",
    "Each neuron computes a weighted sum of its inputs and optionally adds a bias. For example:\n",
    "\n",
    "$$\n",
    "y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "This corresponds to a **fully connected layer** with three inputs and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc328404",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../illustrations/networks2.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5adcb8",
   "metadata": {},
   "source": [
    "We don't need to manually define every weight and bias. Instead, we use PyTorchâ€™s `nn` module, which provides predefined layer types like `nn.Linear`, representing a fully connected (dense) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a linear layer with 3 input features and 1 output\n",
    "lin_layer = nn.Linear(in_features=3, out_features=1)\n",
    "lin_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4194e59",
   "metadata": {},
   "source": [
    "This layer takes a vector of 3 input features and produces a single output. Internally, it maintains:\n",
    "\n",
    "- Three weights, one for each input feature,\n",
    "- One bias term, which acts as an intercept.\n",
    "\n",
    "We can inspect the parameters of the layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa07af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters of the linear layer\n",
    "list(lin_layer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ecaea",
   "metadata": {},
   "source": [
    "The output consists of two tensors:\n",
    "\n",
    "1. A $1 \\times 3$ weight tensor (since we have 3 inputs and 1 output),\n",
    "2. A single bias term.\n",
    "\n",
    "These parameters are initialized randomly. Notice that each parameter has `requires_grad=True`, which means PyTorch will compute gradients with respect to them during backpropagation. This is essential for learning.\n",
    "\n",
    "Next, we wrap this layer in a full model using `nn.Sequential`, which allows us to stack multiple layers into a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the layer in a Sequential model\n",
    "model = nn.Sequential(lin_layer)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de9dba",
   "metadata": {},
   "source": [
    "We have now defined a simple model consisting of a single linear layer. This model behaves exactly like a linear regression function.\n",
    "\n",
    "To make a prediction, we provide input values for the features $x_0, x_1, x_2$. These values need to be passed as a PyTorch tensor with the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input tensor for prediction\n",
    "input_tensor = torch.tensor([3, 2, 5], dtype=torch.float32)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input through the model to get a prediction\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea2d2b",
   "metadata": {},
   "source": [
    "The model returns a prediction based on the current (randomly initialized) weights and bias.\n",
    "\n",
    "Since the model has not been trained yet, the output is arbitrary. In the next section, we will walk through how to optimize these parameters so that the model fits real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a74d9",
   "metadata": {},
   "source": [
    "## 1.4 Practical Example\n",
    "\n",
    "Let us now train the model to fit a simple dataset. We will generate synthetic data that follows a linear relationship with some added noise:\n",
    "\n",
    "$$\n",
    "y = w_0 x + b + \\text{noise}\n",
    "$$\n",
    "\n",
    "Our goal is to learn the parameters $w_0$ and $b$ from this data using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a325ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate input values\n",
    "x_val = torch.arange(0, 10, 0.1)\n",
    "\n",
    "# Generate corresponding output values with noise\n",
    "y_val = 3 + 10 * x_val + 3 * torch.tensor(np.random.randn(len(x_val)), dtype=torch.float32)\n",
    "\n",
    "# Visualize the data\n",
    "plt.plot(x_val, y_val, \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Synthetic Linear Data with Noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bedcae",
   "metadata": {},
   "source": [
    "We created a dataset where the true underlying relationship is:\n",
    "\n",
    "$$\n",
    "y = 3 + 10 \\cdot x + \\text{noise}\n",
    "$$\n",
    "\n",
    "This means the true parameters are $w_0 = 10$ and $b = 3$, but we have added some random noise to make the learning problem more realistic.\n",
    "\n",
    "We will now define a new linear model and train it to recover these parameters from the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new linear model with 1 input and 1 output\n",
    "lin_layer = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "# Show initial parameters (weights and bias)\n",
    "list(lin_layer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea78221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the linear layer in a Sequential model\n",
    "model = nn.Sequential(lin_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d396ab",
   "metadata": {},
   "source": [
    "This model has a single input and a single output. It contains two parameters:\n",
    "\n",
    "- A weight $w_0$ for the input feature,\n",
    "- A bias $b$ (intercept).\n",
    "\n",
    "Both are initialized randomly. We now need to define two components to train the model:\n",
    "\n",
    "1. A **loss function** to measure how well the model performs.\n",
    "2. An **optimizer** that adjusts the parameters based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function: Mean Squared Error (MSE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Use stochastic gradient descent (SGD) with a small learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first input value\n",
    "inputs = x_val[0]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9683ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input to a batch with one sample and one feature\n",
    "inputs = inputs.unsqueeze(0)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19435c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model output for the input\n",
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the corresponding target value and reshape it\n",
    "targets = y_val[0].unsqueeze(0)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5304b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss (mean squared error) for this single example\n",
    "loss = criterion(outputs, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2197c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually compute the squared error to verify the loss\n",
    "((targets[0] - outputs[0]) ** 2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9830a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients via backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one optimization step using the computed gradients\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients to zero (important before the next backward pass)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass after one optimization step\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814281a",
   "metadata": {},
   "source": [
    "We can see that the loss has slightly decreased compared to the initial value. This means the model has started to learn.\n",
    "\n",
    "However, training on a single data point is not sufficient. In practice, we want to use the entire dataset to update the model parameters. PyTorch models can process multiple inputs at once if we format the data correctly â€” as a matrix where each row is a data point and each column is a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a96592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_val to have shape (num_samples, num_features)\n",
    "all_inputs = x_val[:, np.newaxis]\n",
    "\n",
    "# Forward pass for all inputs\n",
    "outputs = model(all_inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce799ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_val similarly to match output shape\n",
    "all_targets = y_val[:, np.newaxis]\n",
    "\n",
    "# Compute the loss over the full dataset\n",
    "loss = criterion(outputs, all_targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e7587",
   "metadata": {},
   "source": [
    "## 1.5 Repeating the Optimization\n",
    "\n",
    "To train the model properly, we repeat the following steps many times:\n",
    "\n",
    "1. Compute predictions from the current model.\n",
    "2. Calculate the loss (error).\n",
    "3. Perform backpropagation to compute gradients.\n",
    "4. Update the model parameters using the optimizer.\n",
    "5. Reset the gradients.\n",
    "\n",
    "This process is repeated for many iterations to gradually minimize the loss. Below, we perform this procedure for 1000 steps and print the loss every 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: repeat optimization steps for 1000 epochs\n",
    "for epoch in range(1000):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(all_inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, all_targets)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f9944",
   "metadata": {},
   "source": [
    "The model starts with a high initial loss around 3000, which reflects the randomly initialized parameters. After training:\n",
    "\n",
    "- At epoch 100, the loss has dropped below 7.35.\n",
    "- The loss continues to decrease slowly and steadily, reaching approximately 7.34 after 900 epochs.\n",
    "\n",
    "This behavior shows that the model is gradually learning the linear relationship between $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values with the trained model\n",
    "pred = model(all_inputs)\n",
    "\n",
    "# Detach the prediction tensor and convert it to a NumPy array for plotting\n",
    "pred_np = pred.detach().numpy()\n",
    "\n",
    "# Plot original data and model predictions\n",
    "plt.plot(x_val, y_val, \"o\", label=\"Noisy Data\")\n",
    "plt.plot(x_val, pred_np, \"r\", label=\"Model Prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Model Fit After Training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2badb66",
   "metadata": {},
   "source": [
    "The plot shows the noisy data points (in blue) and the model's predictions (in red). \n",
    "\n",
    "As the model has been trained on this data, we can see that the predictions closely follow the underlying linear trend, despite the noise added to the data. This indicates that the model has successfully learned the general pattern.\n",
    "\n",
    "However, since we only used a simple linear model, it might not perfectly fit more complex data. In the next sections, we can explore how adding layers or non-linearities can improve the model's capacity to fit more complicated patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72b742",
   "metadata": {},
   "source": [
    "## 1.6 Mini-Batches\n",
    "\n",
    "In the previous sections, we trained the model on the entire dataset at once. While this is fine for small datasets, it is often inefficient for larger datasets. Instead, we can split the data into smaller groups called **mini-batches**. This allows the model to learn from smaller subsets of data at a time, which can speed up training and improve generalization.\n",
    "\n",
    "Mini-batch training is widely used because it strikes a balance between computational efficiency and model accuracy. In the following section, we will modify our training loop to use mini-batches instead of processing the entire dataset at once.\n",
    "\n",
    "To clearly illustrate mini-batch training, let's define and initialize a new model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b23675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fresh linear model for mini-batch training\n",
    "lin_layer_mb = nn.Linear(in_features=1, out_features=1)\n",
    "model_mb = nn.Sequential(lin_layer_mb)\n",
    "\n",
    "# Define loss function and optimizer again\n",
    "criterion_mb = nn.MSELoss()\n",
    "optimizer_mb = optim.SGD(model_mb.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mini-batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Mini-batch training loop\n",
    "for epoch in range(10):  # limited epochs for demonstration\n",
    "    for i in range(0, len(all_inputs), batch_size):\n",
    "        # Select mini-batch\n",
    "        inputs = all_inputs[i : i + batch_size]\n",
    "        targets = all_targets[i : i + batch_size]\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer_mb.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_mb(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_mb(outputs, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameter update\n",
    "        optimizer_mb.step()\n",
    "\n",
    "    # Print loss at the end of each epoch\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392e3b4",
   "metadata": {},
   "source": [
    "The mini-batch training output shows that the loss rapidly decreases during the first few epochs:\n",
    "\n",
    "- The initial loss at epoch 0 is very high (around 3425), reflecting randomly initialized parameters.\n",
    "- Within just a few epochs, the loss significantly decreases, reaching approximately 12 by epoch 4.\n",
    "- After epoch 5, improvements become smaller and the loss stabilizes around 3.5.\n",
    "\n",
    "This demonstrates that training with mini-batches allows the model to update its parameters frequently, leading to rapid initial learning. It also highlights that after a certain point, continuing to train may result in minimal improvements, indicating that the model has reached a stable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with the mini-batch trained model\n",
    "pred_mb = model_mb(all_inputs)\n",
    "\n",
    "# Convert predictions to numpy for plotting\n",
    "pred_mb_np = pred_mb.detach().numpy()\n",
    "\n",
    "# Plot original data and mini-batch trained model predictions\n",
    "plt.plot(x_val, y_val, \"o\", label=\"Noisy Data\")\n",
    "plt.plot(x_val, pred_mb_np, \"r\", label=\"Mini-batch Prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Model Fit After Mini-batch Training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580247",
   "metadata": {},
   "source": [
    "## 1.7 Adding Layers\n",
    "\n",
    "So far, our neural network consisted of just one layer, making it equivalent to simple linear regression. However, neural networks become powerful when we add multiple layers. These additional layers enable the network to capture more complex patterns in the data.\n",
    "\n",
    "Below is an illustration of a neural network with multiple layers:\n",
    "\n",
    "- An input layer with three features,\n",
    "- A hidden layer with two neurons (hidden units),\n",
    "- An output layer producing the final prediction.\n",
    "\n",
    "Each neuron in a layer is connected to every neuron in the next layer. By stacking layers, the network can learn hierarchical representations of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0024625",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../illustrations/networks3.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb51c4d",
   "metadata": {},
   "source": [
    "Let's create a neural network that is slightly more complex than before, with two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269db823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with two fully connected layers\n",
    "lin_layer1 = nn.Linear(in_features=1, out_features=10)\n",
    "lin_layer2 = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "# Wrap the layers in a Sequential container\n",
    "model_2layer = nn.Sequential(lin_layer1, lin_layer2)\n",
    "\n",
    "# Display model architecture\n",
    "model_2layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c5e17",
   "metadata": {},
   "source": [
    "This model consists of two fully connected layers:\n",
    "\n",
    "- The first layer takes a single input and produces 10 outputs (hidden units).\n",
    "- The second layer takes these 10 hidden units as input and produces a single output.\n",
    "\n",
    "Even though this network is deeper than the previous one, it is still a **linear model** because it consists only of linear transformations stacked together. The composition of linear functions is still linear.\n",
    "\n",
    "To model more complex data patterns, we need to introduce **non-linear activation functions** between the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d660c4",
   "metadata": {},
   "source": [
    "## 1.8 Adding Non-Linearities: Activation Functions\n",
    "\n",
    "In many real-world problems, the relationship between inputs and outputs is not linear. Stacking multiple linear layers cannot help in this case, because their composition is still a linear function.\n",
    "\n",
    "To allow the network to learn **non-linear patterns**, we insert a non-linear **activation function** between layers. This function is applied element-wise to the outputs of a layer before passing them to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../illustrations/networks4.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e2493",
   "metadata": {},
   "source": [
    "A commonly used activation function is the **Rectified Linear Unit (ReLU)**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "It outputs the input directly if it is positive, and zero otherwise. Despite its simplicity, ReLU allows neural networks to model complex, non-linear relationships effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098affbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu\n",
    "\n",
    "# Create input values ranging from -10 to 10\n",
    "x = torch.arange(-10.0, 10.1, 0.1)\n",
    "\n",
    "# Apply the ReLU activation\n",
    "y = relu(x)\n",
    "\n",
    "# Plot the ReLU function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_title(\"ReLU Activation Function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"ReLU(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203e412",
   "metadata": {},
   "source": [
    "Let us now apply a neural network with ReLU activations to a non-linear dataset.\n",
    "\n",
    "We will generate data that follows a cosine function with added noise. This kind of data cannot be modeled accurately by a simple linear regression model. By stacking multiple layers with activation functions, we give the network the capacity to approximate this non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c81d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Generate a non-linear dataset: cosine function with noise\n",
    "x_val = torch.arange(0, 10, 0.1)\n",
    "y_val = torch.cos(x_val) + 0.1 * torch.tensor(np.random.randn(len(x_val)), dtype=torch.float32)\n",
    "\n",
    "# Visualize the data\n",
    "sns.scatterplot(x=x_val, y=y_val)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Noisy Cosine Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13357558",
   "metadata": {},
   "source": [
    "To model this non-linear data, we will define a deeper neural network with multiple layers and insert ReLU activation functions between them.\n",
    "\n",
    "The network will consist of:\n",
    "\n",
    "- An input layer with 1 feature,\n",
    "- Two hidden layers with 64 units each, using ReLU activations,\n",
    "- An output layer with 1 unit to predict the target value.\n",
    "\n",
    "This architecture allows the model to approximate complex non-linear functions without the need for manual feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a deeper neural network with ReLU activations\n",
    "lin_layer1 = nn.Linear(in_features=1, out_features=64)\n",
    "lin_layer2 = nn.Linear(in_features=64, out_features=64)\n",
    "lin_layer3 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "# Combine layers with activations using Sequential\n",
    "model_nonlinear = nn.Sequential(lin_layer1, nn.ReLU(), lin_layer2, nn.ReLU(), lin_layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b238600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer for the non-linear model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model_nonlinear.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the first half of the data (50 points), using mini-batches of size 10\n",
    "for epoch in range(10000):\n",
    "    for i in range(5):  # 5 batches of 10 samples each\n",
    "        # Select inputs and targets for the batch\n",
    "        inputs = x_val[i * 10 : (i + 1) * 10].unsqueeze(1)\n",
    "        targets = y_val[i * 10 : (i + 1) * 10].unsqueeze(1)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_nonlinear(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and parameter update\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions over the full dataset\n",
    "predicted_torch = model_nonlinear(x_val.unsqueeze(1))\n",
    "pred = predicted_torch.detach().numpy().ravel()\n",
    "\n",
    "# Plot original data and model prediction\n",
    "sns.scatterplot(x=x_val, y=y_val, label=\"Data\")\n",
    "plt.plot(x_val, pred, \"r-\", label=\"Model Prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Neural Network Fit on Non-Linear Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295f258",
   "metadata": {},
   "source": [
    "The plot shows that the neural network is able to fit the first half of the data very well. Since the model was only trained on the first 50 points, it generalizes poorly to the second half of the data, where the prediction diverges.\n",
    "\n",
    "This behavior highlights an important limitation of deep learning models: they tend to **memorize** the training data and may not generalize well to unseen inputs unless trained carefully. In contrast to hand-crafted models with strong assumptions (such as periodicity), neural networks rely purely on data-driven learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1186709",
   "metadata": {},
   "source": [
    "## 1.9 Overfitting\n",
    "\n",
    "A common issue in machine learning, especially with neural networks, is **overfitting**. Overfitting occurs when a model learns the training data too well, including its noise or random fluctuations, at the cost of generalizing poorly to new data.\n",
    "\n",
    "This problem becomes more likely when:\n",
    "\n",
    "- The model is very complex (many layers or parameters),\n",
    "- The training dataset is small,\n",
    "- Training continues for too many epochs without regularization.\n",
    "\n",
    "To monitor for overfitting, we typically evaluate model performance not only on the training data but also on a separate **test set** that the model never sees during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(40)\n",
    "\n",
    "# Create small noisy linear dataset\n",
    "x_val = torch.linspace(0, 10, 30)\n",
    "y_val = 1 * x_val + 1.5 * torch.tensor(np.random.randn(len(x_val)), dtype=torch.float32)\n",
    "\n",
    "# Wrap in a DataFrame\n",
    "dataset = pd.DataFrame({\"x_val\": x_val, \"y_val\": y_val})\n",
    "\n",
    "# Split into training and testing sets\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.32, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "sns.scatterplot(x=dataset[\"x_val\"], y=dataset[\"y_val\"])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Noisy Linear Data (Small Sample)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a large model for a small dataset\n",
    "lin_layer1 = nn.Linear(in_features=1, out_features=64)\n",
    "lin_layer2 = nn.Linear(in_features=64, out_features=64)\n",
    "lin_layer3 = nn.Linear(in_features=64, out_features=64)\n",
    "lin_layer4 = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "# Stack layers with ReLU activations\n",
    "model_overfit = nn.Sequential(lin_layer1, nn.ReLU(), lin_layer2, nn.ReLU(), lin_layer3, nn.ReLU(), lin_layer4)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model_overfit.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_train = []\n",
    "errors_test = []\n",
    "\n",
    "# Convert training and testing data to tensors\n",
    "x_train = torch.tensor(dataset_train[[\"x_val\"]].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(dataset_train[[\"y_val\"]].values, dtype=torch.float32)\n",
    "x_test = torch.tensor(dataset_test[[\"x_val\"]].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(dataset_test[[\"y_val\"]].values, dtype=torch.float32)\n",
    "\n",
    "# Train for many epochs to demonstrate overfitting\n",
    "for epoch in range(20000):\n",
    "    for i in range(0, len(x_train), 10):\n",
    "        inputs = x_train[i : i + 10]\n",
    "        targets = y_train[i : i + 10]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_overfit(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model on train and test sets\n",
    "    with torch.no_grad():\n",
    "        train_pred = model_overfit(x_train)\n",
    "        test_pred = model_overfit(x_test)\n",
    "        train_loss = mean_squared_error(y_train.numpy(), train_pred.numpy())\n",
    "        test_loss = mean_squared_error(y_test.numpy(), test_pred.numpy())\n",
    "        errors_train.append(train_loss)\n",
    "        errors_test.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and test loss over time\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors_train, label=\"Train Loss\")\n",
    "ax.plot(errors_test, label=\"Test Loss\")\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_title(\"Training vs Test Loss\")\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946222c4",
   "metadata": {},
   "source": [
    "The plot illustrates a typical case of overfitting:\n",
    "\n",
    "- In the early stages of training, both training and test losses decrease, indicating that the model is learning a useful pattern.\n",
    "- After some point, the **training loss continues to decrease**, but the **test loss starts to increase**. This means the model is starting to memorize the training data, including its noise, rather than learning a generalizable pattern.\n",
    "\n",
    "The increasing test loss is a clear sign that the model is **overfitting**. It performs well on data it has seen but generalizes poorly to new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "pred_train = model_overfit(x_train).detach().numpy().ravel()\n",
    "pred_test = model_overfit(x_test).detach().numpy().ravel()\n",
    "\n",
    "# Plot predictions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set\n",
    "sns.scatterplot(x=dataset_train[\"x_val\"], y=dataset_train[\"y_val\"], ax=ax[0], label=\"Train Data\")\n",
    "ax[0].plot(dataset_train[\"x_val\"], pred_train, \"ro\", alpha=0.6, label=\"Prediction\")\n",
    "ax[0].set_title(\"Train Set\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Test set\n",
    "sns.scatterplot(x=dataset_test[\"x_val\"], y=dataset_test[\"y_val\"], ax=ax[1], label=\"Test Data\")\n",
    "ax[1].plot(dataset_test[\"x_val\"], pred_test, \"ro\", label=\"Prediction\")\n",
    "ax[1].set_title(\"Test Set\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc412fc1",
   "metadata": {},
   "source": [
    "The plots show that the model fits the training data very closely, even matching noisy fluctuations. On the test set, however, the predictions are far less accurate and deviate significantly from the actual data points.\n",
    "\n",
    "This confirms that the model has overfitted: it learned the training data too precisely but failed to generalize to new, unseen examples.\n",
    "\n",
    "In practice, overfitting can be reduced through techniques such as:\n",
    "\n",
    "- Using more training data,\n",
    "- Reducing model complexity,\n",
    "- Adding regularization (e.g., weight decay, dropout),\n",
    "- Stopping training early based on validation performance.\n",
    "\n",
    "Understanding overfitting is essential for building models that perform well in real-world settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97afe705",
   "metadata": {},
   "source": [
    "## 1.10 Exercise: Predict Insurance Cost from Age\n",
    "\n",
    "In this exercise, you'll build a very simple neural network to predict a person's **medical insurance charges** based on their **age**.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "- Filter the dataset so that it only contains non-smokers\n",
    "- Use age as the only input feature\n",
    "- Predict the insurance cost (charges)\n",
    "- Build a small neural network (e.g., one hidden layer)\n",
    "- Train the model and visualize the results\n",
    "\n",
    "This is a regression task â€” the output is a continuous number (insurance cost in USD). Part of the code is already implemented. **You need to fill out the parts that are marked with TODO comments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Plot data\n",
    "sns.scatterplot(df, x='age', y='charges', hue='smoker')\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Insurance Charges (USD)\")\n",
    "plt.title(\"Insurance Charges vs. Age\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04846f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter the dataset to only include non-smokers\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select input and target\n",
    "X_train = df_train[[\"age\"]]\n",
    "y_train = df_train[[\"charges\"]]\n",
    "X_test = df_test[[\"age\"]]\n",
    "y_test = df_test[[\"charges\"]]\n",
    "\n",
    "# Normalize features\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "X_test = feature_scaler.transform(X_test)\n",
    "\n",
    "# Normalize target\n",
    "target_scaler = StandardScaler()\n",
    "y_train = target_scaler.fit_transform(y_train)\n",
    "y_test = target_scaler.transform(y_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad27a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# TODO: Define a simple feedforward neural network with a non-linear activation function\n",
    "model = ...\n",
    "\n",
    "# TODO: Define loss function (MSE) and optimizer (SGD) with a learning rate of 0.001\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # TODO: Compute the loss\n",
    "    loss = ...\n",
    "\n",
    "    # Set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # TODO: Perform a backward pass and an optimization step\n",
    "    ...\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/1000], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9607ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    # TODO: Make predictions on the test set and compute the loss\n",
    "    y_pred = ...\n",
    "    test_loss = ...\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_scaler.inverse_transform(X_test.numpy())\n",
    "predictions = target_scaler.inverse_transform(y_pred.numpy())\n",
    "\n",
    "# TODO: Plot the predictions against the actual values\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
