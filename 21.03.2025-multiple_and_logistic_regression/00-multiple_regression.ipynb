{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22cf5af-a9c3-4f63-aec7-e0b6632caf1b",
   "metadata": {},
   "source": [
    "# Regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7c089-6238-42b4-aad4-ff763bd421f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a5f24-b31c-4cbd-abdc-7aedfcaf62dc",
   "metadata": {},
   "source": [
    "So far, we've looked at cases where our dependent variable—the value we're trying to predict—depends on just one feature. For example, predicting `fixed acidity` from `density`. However, as mentioned previously, regression models can easily handle multiple features:\n",
    "\n",
    "$$y = w_0 + w_1 \\cdot \\texttt{var}_1 + w_2 \\cdot \\texttt{var}_2 + \\dots$$\n",
    "\n",
    "All algorithms provided by scikit-learn naturally support this scenario. Let's now try predicting the wine's `fixed acidity` based on both `density` and `pH`. To do this, we need to adjust our `X` variable to include multiple features. We'll run the regression twice—once using a single feature, and once using two features—to compare the results clearly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6d303-30b5-4f0b-be8e-84637485946f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820d360-1c52-4ccb-95ec-e767e40f1292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a9668-0b00-4e2e-bce6-ab3dfba98bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The target (dependent) variable stays the same across both experiments\n",
    "y = wine['fixed acidity']\n",
    "\n",
    "# Experiment 1: Using only one feature\n",
    "X_1 = wine[['density']]\n",
    "\n",
    "# Experiment 2: Using two features\n",
    "X_2 = wine[['density', 'pH']]\n",
    "X_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23a79b-dfd8-4d93-b606-2e30efe02169",
   "metadata": {},
   "source": [
    "Now we can fit a model. We'll perform this twice: first using only the `density` feature, and then using both `density` and `pH`, so we can directly compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a9230-af24-4cdb-be3e-fc95cf2997c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate two different linear regression models\n",
    "model_one = linear_model.LinearRegression()\n",
    "model_two = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the models\n",
    "res_one = model_one.fit(X=X_1, y=y)\n",
    "res_two = model_two.fit(X=X_2, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03c277-02b8-40ea-b3c4-cd5e14543f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coefficients learned by model one\n",
    "res_one.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335bf48-2148-46e8-98ef-8211631927d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coefficients learned by model two\n",
    "res_two.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf505ce7-ea9f-405d-826d-4cfb5fcb86e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Intercept learned by model two\n",
    "res_two.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71ed32-20e8-4bc6-89f7-ae59d842abe4",
   "metadata": {},
   "source": [
    "We see that the second model has two values in `coef_`, one for each feature (`density` and `pH`). Notice also that the coefficient $w_1$ for `density` has changed (454 compared to 662 previously). This difference is expected, since we're now optimizing the regression using two variables simultaneously.\n",
    "\n",
    "Next, let's compare how this affects the overall error by calculating the sum of squared residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436f1a3-16f6-499c-bf72-357e08255bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate predictions using model one and compute the mean squared error in one line\n",
    "mse = mean_squared_error(res_one.predict(X_1), y)\n",
    "\n",
    "# Print the mean squared error with three decimal places\n",
    "print(f\"Mean Squared Error for model one: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d2e47-c79c-4004-9f8c-376343a6cf1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do the same for model two - we overwrite the variable mse since we don't need the old value anymore\n",
    "mse = mean_squared_error(res_two.predict(X_2), y)\n",
    "\n",
    "print(f\"Mean Squared Error for model two: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bedd70-3cdb-43f6-9800-c9df0b21b5eb",
   "metadata": {},
   "source": [
    "Clearly, the fit using two variables captures more of the underlying information! Our model now represents a surface described by the function $z = f(x, y)$, where $z$ is the predicted `fixed acidity`, and the features $x$ and $y$ correspond to `density` and `pH`, respectively.\n",
    "\n",
    "We can still visualize our model by creating two-dimensional plots for each feature separately. To do this, we simply ignore (or project out) one of the two features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee75c6-69d4-4633-946c-4306f27d969b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots (one row, two columns)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Scatter plot showing the relationship between density and fixed acidity\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity', label='data', ax=ax[0])\n",
    "ax[0].plot(X_2.density, res_two.predict(X_2), 'ro', alpha=0.5, label='prediction')\n",
    "\n",
    "# Scatter plot showing the relationship between pH and fixed acidity\n",
    "sns.scatterplot(data=wine, x='pH', y='fixed acidity', label='data', ax=ax[1])\n",
    "ax[1].plot(X_2.pH, res_two.predict(X_2), 'ro', alpha=0.5, label='prediction')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa4ebf-942c-4aa9-a97f-a990b0d6a624",
   "metadata": {},
   "source": [
    "We can also visualize our data and predictions together in a **3D scatter plot**, where the first two axes represent the features `density` and `pH`, and the third axis shows our target `fixed acidity`. Let's do this to see how the two-dimensional views we previously plotted can be represented in three dimensions.\n",
    "\n",
    "We'll create a grid of combined `density` and `pH` values, predict the corresponding `fixed acidity` values, and plot these predictions. You'll notice that the predictions form a flat surface (plane) in 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81996187-b21b-4807-abf5-0fa315a1414c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative way to create a figure, we will add two subplots to this figure later\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "X_1_grid, X_2_grid = np.meshgrid(\n",
    "    # Here we use `np.linspace` instead of `np.arange`. In this case, the last argument is the number of points we want to generate and not the step size.\n",
    "    np.linspace(wine.density.min(), wine.density.max(), 20),\n",
    "    np.linspace(wine.pH.min(), wine.pH.max(), 20),\n",
    ")\n",
    "\n",
    "# The `ravel` operation transforms the 2D arrays into 1D arrays by concatenating all rows.\n",
    "X_1_grid = X_1_grid.ravel()\n",
    "X_2_grid = X_2_grid.ravel()\n",
    "\n",
    "# Predict the values for the different combinations of density and pH\n",
    "y_pred = res_two.predict(pd.DataFrame({'density': X_1_grid, 'pH': X_2_grid}))\n",
    "\n",
    "# The actual values\n",
    "x_1 = X_2.density.values\n",
    "x_2 = X_2.pH.values\n",
    "x_3 = y.values\n",
    "\n",
    "# Create two subplots in the figure\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "axes = [ax1, ax2]\n",
    "\n",
    "for ax in axes:\n",
    "    # Plot the surface of the actual values in the dataset\n",
    "    surf = ax.scatter(x_1, x_2, x_3, linewidth=0)\n",
    "\n",
    "    # Plot the surface of the predicted values across the grid\n",
    "    surf = ax.scatter(X_1_grid, X_2_grid, y_pred, linewidth=0)\n",
    "\n",
    "    ax.set_xlabel('density', fontdict={'fontsize': 12})\n",
    "    ax.set_ylabel('pH', fontdict={'fontsize': 12})\n",
    "    ax.set_zlabel('fixed acidity', fontdict={'fontsize': 12})\n",
    "\n",
    "# Rotate the view\n",
    "ax2.view_init(elev=0., azim=0)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cf352-1d8e-4947-b63b-32dd7c21d015",
   "metadata": {},
   "source": [
    "## Features on Different Scales\n",
    "\n",
    "When working with multiple features, a common issue is that they often have very different scales. In the `wine` dataset, for instance, there's roughly an order-of-magnitude difference between `fixed acidity` and `volatile acidity`. Since regression models typically minimize the overall distance between predicted and actual values across *all* features simultaneously, features with larger magnitudes can disproportionately affect the result.\n",
    "\n",
    "For example, if we switch from `LinearRegressor` to a `Ridge` regressor without scaling our data first, the model fitting might fail or produce incorrect results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c622504-1ac4-424b-a393-2f45befcdd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a Ridge regression model instead of a linear regression model\n",
    "model = linear_model.Ridge()\n",
    "res_ridge = model.fit(X=X_2, y=y)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "# Scatter plot showing the relationship between density and fixed acidity\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity', label='data', ax=ax[0])\n",
    "\n",
    "# Plot the predictions of the Ridge regression model against the density\n",
    "ax[0].plot(X_2.density, res_ridge.predict(X_2), 'ro', alpha=0.5, label='prediction')\n",
    "\n",
    "# Scatter plot showing the relationship between pH and fixed acidity\n",
    "sns.scatterplot(data=wine, x='pH', y='fixed acidity', label='data', ax=ax[1])\n",
    "\n",
    "# Plot the predictions of the Ridge regression model against the pH\n",
    "ax[1].plot(X_2.pH, res_ridge.predict(X_2), 'ro', alpha=0.5, label='prediction')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc471541-ad6e-454a-b966-65f2dec1d5f1",
   "metadata": {},
   "source": [
    "Ridge regression helps prevent individual features from disproportionately influencing the model, which could lead to overfitting or misinterpretation. To ensure each feature contributes equally, we typically **normalize** them by adjusting their scales. Normalization sets each feature's mean to 0 and its standard deviation to 1.\n",
    "\n",
    "In the example below, we have two variables: one centered around 4 with a spread (standard deviation) of 1, and another centered around 10 with a spread of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3ac46-afce-4b26-9fe4-aa8074890401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can create a new dataframe by specyfing the column names and values unside a dictionary\n",
    "variables = pd.DataFrame(\n",
    "    {\n",
    "        'variable 1': np.random.normal(loc=10, scale=5, size=1000),\n",
    "        'variable 2': np.random.normal(loc=4, scale=1, size=1000)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Jointplot is a figure-level function, it will automatically create a new figure for us\n",
    "jpl = sns.jointplot(data=variables, x='variable 1', y='variable 2', xlim=(-10, 30), ylim=(-10, 30))\n",
    "\n",
    "# Set the size of the figure\n",
    "jpl.figure.set_size_inches(w=4, h=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814b60f-990a-4494-81a6-db7431543156",
   "metadata": {},
   "source": [
    "The goal is to transform these points so that they have similar distributions. For that we have to shift them (subtraction) so that they are centered around 0 and divide them by a certain factor so that they have the same spread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cdda5c-8d6d-499e-a326-282b850d65b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables['variable 1'] = (variables['variable 1'] - np.mean(variables['variable 1'])) / np.std(variables['variable 1'])\n",
    "variables['variable 2'] = (variables['variable 2'] - np.mean(variables['variable 2'])) / np.std(variables['variable 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd0bca-ddae-492e-87b8-08e1a7d352f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jpl = sns.jointplot(data=variables,\n",
    "              x='variable 1', y='variable 2', xlim=(-10,30), ylim=(-10,30)\n",
    ")\n",
    "jpl.figure.set_size_inches(w=4, h=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc48187-31c3-49a6-bb02-d5621e56b2d2",
   "metadata": {},
   "source": [
    "We see that now the two variables have similar distributions centered around 0 and with a spead of 1. Instead of doing this operation manually we can use a ```preprocessing``` function from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977e0f0-21d1-43b3-9835-3d2c9350c793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0adaa-8523-41f4-9510-0bfba53a2a99",
   "metadata": {},
   "source": [
    "Here we use the `StandardScaler` to normalize our data. Just like our models, the scaler is used in two steps:\n",
    "\n",
    "1. **Instantiate** the scaler.\n",
    "2. **Fit and transform** the data. In this step, we calculate the mean and standard deviation and then normalize our data accordingly. Since we perform both operations, we use the `fit_transform` method instead of just `fit`.\n",
    "\n",
    "We can apply `StandardScaler` directly to our entire DataFrame. Because it returns a NumPy array, we convert the result back into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc4e7e-05a3-47a3-a5b1-70c48ec8bbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the StandardScaler\n",
    "sc = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler to the wine dataset and transform it, then convert it back to a DataFrame\n",
    "wine_scaled = pd.DataFrame(sc.fit_transform(wine), columns=wine.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec531ad5-ae5c-482d-84e8-01420caa623b",
   "metadata": {},
   "source": [
    "Let's make a histogram of before/after normalization of the ```pH```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20670801-06c8-4acb-9d03-850539955935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# Before normalization\n",
    "sns.histplot(wine.pH, ax=ax[0])\n",
    "ax[0].set_title('Not normalized')\n",
    "\n",
    "# After normalization\n",
    "sns.histplot(wine_scaled.pH, ax=ax[1])\n",
    "ax[1].set_title('Normalized');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f304a-ede4-4727-8664-67140f77a7b1",
   "metadata": {},
   "source": [
    "We see that it doesn't affect the shape of the distribution, just its scale (see the `pH` values on the x-axis). Now we can try to fit a Ridge regression model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759cfe7-ed18-4b27-bb03-b911eef4aa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_2_scaled = wine_scaled[['density', 'pH']]\n",
    "y_scaled = wine_scaled['fixed acidity']\n",
    "\n",
    "model = linear_model.Ridge()\n",
    "\n",
    "res_ridge = model.fit(X=X_2_scaled, y=y_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "sns.scatterplot(data=wine_scaled, x='density', y='fixed acidity', label='data', ax=ax[0])\n",
    "ax[0].plot(X_2_scaled.density, res_ridge.predict(X_2_scaled), 'ro', alpha=0.5, label='prediction')\n",
    "\n",
    "sns.scatterplot(data=wine_scaled, x='pH', y='fixed acidity', label='data', ax=ax[1])\n",
    "ax[1].plot(X_2_scaled.pH, res_ridge.predict(X_2_scaled), 'ro', alpha=0.5, label='prediction')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bbbb4",
   "metadata": {},
   "source": [
    "We see that now the model was able to learn the relationship as with the linear regression before because we are not giving more weight to a particular feature anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc262b-e362-42d8-abb7-dc5eac97bed8",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Import the `housing.csv` dataset from the datasets directory. This dataset contains information about houses sold in Kings County (California) such as price, surface etc. **Note** that this is not exactly the same dataset as `kc_house_data.csv`!\n",
    "\n",
    "2. Predict the ```price``` using first only the ```grade``` and then the ```grade```, ```bedrooms``` and ```bathrooms``` columns\n",
    "\n",
    "3. Compute the mean square error for both models. Which one is better and how big is the difference?\n",
    "\n",
    "4. Make a pairplot with the ```bedrooms```, ```grade``` and ```price``` columns. Do you see anything special regarding the ```bedrooms``` data? If yes try to fix it and rerun the above analysis. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
