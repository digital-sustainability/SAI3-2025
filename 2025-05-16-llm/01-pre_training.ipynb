{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "- This code demonstrates a toy example of how LLM pretraining works in a very simplified way.\n",
    "- Your task is to complete the empty cells and fill the missing parts of the code indicated using the ellipsis \"...\"\n",
    "\n",
    "Through this exercise you will learn:\n",
    "- What a vocabulary construction looks like for a given dataset\n",
    "- How tokenization can be done\n",
    "- Encoding the data before training and decoding the data after generation\n",
    "- Loss function most commonly used\n",
    "- Making a forward pass\n",
    "- Training elements like optimizers </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Below demostrates a toy example that takes a text and used each character in the text as a \"token\". The code even if it works for you will probably not generate anything legible. The goal is that you should understand each element of the pretraining process. In reality, the training is a lot more sophisticated for many reasons - some being scale of the datasets, size of the models etc. The fundamentals on which these models are trained, however, can be demonstrated using this toy example. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiTfAZLnandu"
   },
   "outputs": [],
   "source": [
    "# read file llm_pretraining.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em7IOTMGasmY"
   },
   "outputs": [],
   "source": [
    "# create vocabulary, make sure the vocabulary is stored in a variable called \"chars\" (used later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTUIp-qKayW9"
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTUIp-qKayW9"
   },
   "outputs": [],
   "source": [
    "encoding_example = encode(\"hii there\")\n",
    "encoding_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTUIp-qKayW9"
   },
   "outputs": [],
   "source": [
    "decode(encoding_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFYDtK1MbLuv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWm9tu6ebH5r"
   },
   "outputs": [],
   "source": [
    "# encode the dataset, make sure that after encoding the output is stored in a variable named \"data\" (used later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLEBI5f9bJOi"
   },
   "outputs": [],
   "source": [
    "# training and test data split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[:n]\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ks1zTIEBbWJT"
   },
   "outputs": [],
   "source": [
    "# batching function\n",
    "# analyse this and see if you able to understand what is going on\n",
    "def get_batch(split):\n",
    "    data_ = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(yb)\n",
    "\n",
    "# bigram models with the appropriate functions\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size_):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size_, vocab_size_)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = ... # cross entropy loss function\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOeapmFibcCW",
    "outputId": "b53ffdaf-c779-4c86-f72e-6d1ffc52f73e"
   },
   "outputs": [],
   "source": [
    "# initialize the BigramLanguageModel, make sure the variable name is \"m\" (used later)\n",
    "# make a forward pass with the BigramLanguageModel Class, this step is just a test to check if the call works and not part of the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFbfIrpAbfqm"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "optimizer = ... # initialise the optimiser\n",
    "batch_size = ... # set a batch size\n",
    "for steps in range(1000):  # 1000 steps training, you may change this\n",
    "    xb, yb = ... # get the training batch\n",
    "\n",
    "    logits, loss =  # make the forward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()  # backpropagation\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z3RgcZ5poqN"
   },
   "outputs": [],
   "source": [
    "# this step generates text using your trained model\n",
    "# it probably won't generate anything interesting (or maybe it will?)\n",
    "# by the time you reach this step you should have uncerstood the principals of:\n",
    "# 1. How the pre-training works\n",
    "# 2. Can go back to the corret sources to understand how to expand on this basic knowledge\n",
    "\n",
    "input_data = torch.tensor([encode(\"Let us kill him\")], dtype=torch.long)  # you can change the encoding sentence to something else\n",
    "generate_n_tokens = 500 # you can change max_new_tokens=500 to change size of the generation\n",
    "print(decode(m.generate(idx=input_data, max_new_tokens=generate_n_tokens)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
