{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479455eb-d3c5-4282-9144-adaba9fc6bab",
   "metadata": {},
   "source": [
    "# Exploring your data\n",
    "\n",
    "EDA stands for *Exploratory Data Analysis*. It is an important step of any project that will give you an idea of the contents of your dataset so that that you can decide on what method to use to extract the relevant information. There are two parts in EDA: first you have to verify the content and formatting of your data and second you need to visualize it to get some insight into relations between variables and their distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec899c6-7275-45c3-8697-e4d4888ff22d",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Of course the first task is to import the dataset or access it. In this course we always import simple csv files or have a folder full with images that we can import. However in most professional cases, the dataset is embedded in a database and you might need to do some work to get access to the data.\n",
    "\n",
    "To illustrate the first steps of EDA we import here a small \"made-up\" dataset that make it easy to understand potential problems. It just contains some information about classical composers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279a269-eadd-47d6-a2fa-be7cd2daeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Silence specific warnings\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a223086-96ef-4557-8996-2422e627aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers = pd.read_excel(\"../datasets/composers.xlsx\", sheet_name=1)\n",
    "composers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0448b-af7d-458e-8039-704eaffb3e25",
   "metadata": {},
   "source": [
    "## Checking the data\n",
    "\n",
    "The first thing we have to check is the type of data we have in the table. We can easily do this with the ```dtypes``` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39bee5-77af-4767-bbb4-c7d3489a0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b0000-511f-4841-bff6-3b48c318bd7d",
   "metadata": {},
   "source": [
    "We see that the death column is an *object* column, which means non-numerical or text. However it should really be a number! The problem is that some values are missing, and Pandas doesn't know what to do with those. The birth column also has a missing value, but it is recognized as *Not a Number* and Pandas can deal with it. The death column in contrast has a text value ```unknown```. How can we fix this? We can for example ```replace``` some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568d0b9-cf57-4403-adc4-86cae91f97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.death.replace('unknown', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa925129-678f-49be-ab0e-39b1cd632783",
   "metadata": {},
   "source": [
    "Of course we actually need to assign these new column to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4820f-e0ea-4c23-95d2-d943676fae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.death = composers.death.replace('unknown', np.nan)\n",
    "composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22b22e-5be8-4442-97d8-7f01bce9db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342fbcd-83d5-4c99-a7ca-0b0d0f5d957b",
   "metadata": {},
   "source": [
    "We can see that the column is still of type `object`. We can map both the birth and the death column to integers, which in this case is better than float because it will save some space and we are only interested in discrete years. However, we cannot just use the normal `int` python datatype because it does not support NaNs. We could either go back to using a float, or we use a Pandas datatype such as the [16-bit Nullable Interger](https://pandas.pydata.org/docs/reference/api/pandas.Int16Dtype.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42740051-1aed-4406-923c-9a4b3276a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.birth = composers.birth.astype(pd.Int16Dtype())\n",
    "composers.death = composers.death.astype(pd.Int16Dtype())\n",
    "\n",
    "composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee65368-7a1e-471e-8546-bc311d4aeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf701b6-480f-4335-9325-2ab29d6c0e6f",
   "metadata": {},
   "source": [
    "Now we preserved the NaN (or <NA>) values. Do we actually need them? In some cases we can just leave them and they are just discarded. For example we can ask Pandas to compute the mean of the columns and it just discards those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8468b-7dea-44c9-82e8-53a801fc407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bab043-36a5-4323-a7fc-6e33a496c035",
   "metadata": {},
   "source": [
    "If there are only a few values and we made sure they are not \"important\" (e.g. they do not represent a very specific class of data), we can just discard them. Again, we can use Pandas for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc5464-bf52-4125-a7fd-03b0356b5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "composers = composers.dropna()\n",
    "composers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcac026-db15-492b-88e5-1bd7600658cc",
   "metadata": {},
   "source": [
    "## Visualize relations between data\n",
    "\n",
    "To understand relationships between features as well as their distributions we can could plot histograms and scatter plots for all of them. Instead of doing this manually, we can use a very useful Seaborn function called pair-plot. To get interesting plots we now turn back to our wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818a385-b159-4842-9397-52810b0d2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b8b16-48ad-4c4b-af3a-10a726413d1b",
   "metadata": {},
   "source": [
    "We use ```sns.pairplot``` only on a few columns of the dataset so that we can better see the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334af66-e97c-4d5c-8147-0f408f4e61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.pairplot(wine.iloc[:,[0, 1, 2, 3, -1]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bb00f-6542-4031-89d2-75606f31cb48",
   "metadata": {},
   "source": [
    "As you can see this provides us on the diagonal a histogram for each feature in the DataFrame and at other positions a scatter plot for all possible pairs of variables. This type of plot can give us visually already a lot of information. For example:\n",
    "\n",
    "- The pairplot displays histograms along the diagonal, showing the distribution of each feature individually, while the off-diagonal plots are scatter plots that visualize relationships between every pair of features. This type of visualization helps us quickly identify patterns and potential issues in the dataset.\n",
    "- Fixed acidity and citric acid appear to be highly correlated. Since they provide similar information, we might consider removing one to simplify the dataset without losing much predictive power.\n",
    "- The histogram of residual sugar is right-skewed, with most values concentrated at lower levels and a long tail extending to the right. This suggests the presence of outliers—likely sweet wines—which could distort our analysis if not handled properly.\n",
    "- Residual sugar outliers influence multiple relationships in the dataset. For example, when examining how sugar relates to fixed acidity, those extreme values could disproportionately affect correlation estimates. This is something we should account for in modeling.\n",
    "- Citric acid has a disproportionately large number of very low (or zero) values. This suggests that many wines in the dataset contain little or no citric acid. Such a sharp cutoff might indicate a data collection artifact or a categorical effect, which could introduce bias in our models.\n",
    "- The quality feature is highly imbalanced. Most wines are rated 5 or 6, while there are relatively few with ratings of 4 or 7. This imbalance is important to consider when building a predictive model, as a naïve classifier that always predicts 5 or 6 would achieve high accuracy but fail to provide meaningful predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce34016-c9d3-4500-b482-0e8a3e186e29",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "As next step, we might want to correct for some of the above observations (e.g. remove outliers). We will see practical example later on when we try to use ML methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e49296-9349-43b9-9a30-79bdcdf90eef",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Import the dataset `kc_house_data.csv` from the dataset directory. It is a dataset about the price of houses in California with information such as number bedrooms, surface etc..\n",
    "\n",
    "2. Use the ```pairplot``` function to looks at relations between variables. Use only the columns 1 to 8 to avoid having too many plots (ignore the first column which is only an index).\n",
    "\n",
    "3. What do you observe in the relation between the variables ```price``` vs ```sqft_living```? Do you think you can predict the price with the ```sqft_living``` variable?\n",
    "\n",
    "4. The bedroom distribution (on the diagonal) is strange. Make a single histogram (```sns.histplot```) with just this variable. Does the plot look ok? If not what can you try to adjust and why?\n",
    "\n",
    "5. Do we have the same number of houses with all number of bedrooms? If not, how could this be a problem in the frame of Machine Learning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
