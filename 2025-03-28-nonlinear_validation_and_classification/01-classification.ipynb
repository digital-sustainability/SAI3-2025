{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In the previous chapter, we learned about logistic regression—a natural starting point for classification due to its close connection with linear regression. Here, we revisit the movement dataset and show how logistic regression can be applied using a single feature. \n",
    "\n",
    "We'll use the acceleration along the Z-axis (`z_acc`) to predict the type of exercise, encoded as 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "movement = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/digital-sustainability/SAI3-2025/refs/heads/main/datasets/movement.csv\"\n",
    ")\n",
    "movement.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fitting a logistic regression model using only one feature: `z_acc`. The target variable is `move_type`, which indicates the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X = movement[[\"z_acc\"]]\n",
    "y = movement[\"move_type\"]\n",
    "\n",
    "log_model = linear_model.LogisticRegression()\n",
    "log_model.fit(X=X, y=y)\n",
    "\n",
    "X_pred = pd.DataFrame(np.arange(-10, 50, 0.1), columns=[\"z_acc\"])\n",
    "pred_prob = log_model.predict_proba(X_pred)\n",
    "pred = log_model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the decision boundary of the logistic regression model. While the model predicts a category based on `z_acc`, we will see that it cannot perfectly separate the two classes with this feature alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=movement, x=\"z_acc\", y=\"move_type\")\n",
    "ax.plot(np.arange(-10, 50, 0.1), pred, \"g\", label=\"Predicted class\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Two Features\n",
    "\n",
    "With only one feature (`z_acc`), we saw that the logistic regression model couldn't perfectly separate the two exercise types. What if we include an additional feature?\n",
    "\n",
    "We'll now use both `z_acc` and `y_acc` to fit a logistic regression model. To visualize the data in this higher-dimensional feature space, we'll plot it in 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "\n",
    "axes = [ax1, ax2]\n",
    "for ax in axes:\n",
    "    scatter = ax.scatter(\n",
    "        movement[\"z_acc\"].values,\n",
    "        movement[\"y_acc\"].values,\n",
    "        movement[\"move_type\"].values,\n",
    "        c=movement[\"move_type\"].values,\n",
    "        cmap=\"Set1\",\n",
    "    )\n",
    "    ax.set_xlabel(\"z_acc\", fontsize=12)\n",
    "    ax.set_ylabel(\"y_acc\", fontsize=12)\n",
    "    ax.set_zlabel(\"move_type\", fontsize=12)\n",
    "\n",
    "# 3D View (angled and top-down)\n",
    "ax1.view_init(elev=30.0, azim=20)\n",
    "ax2.view_init(elev=90.0, azim=-90)\n",
    "ax2.set_proj_type(\"ortho\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 3D plots show how adding `y_acc` as a second feature helps distinguish the two classes more effectively. While neither feature alone provides a clear separation, their combination makes a linear decision boundary more feasible.\n",
    "\n",
    "Let’s now fit a logistic regression model using both features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = movement[[\"z_acc\", \"y_acc\"]]\n",
    "y = movement[\"move_type\"]\n",
    "\n",
    "log_model = linear_model.LogisticRegression()\n",
    "log_model.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Sigmoid Decision Surface\n",
    "\n",
    "When using two features, the logistic regression model defines a **sigmoid-shaped surface** in 3D space. The decision boundary (i.e., where predicted probability is 0.5) becomes a **plane**.\n",
    "\n",
    "Let’s plot this surface and highlight the region where the model is uncertain (around $p=0.5$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of points spanning the input space\n",
    "X_1_grid, X_2_grid = np.meshgrid(\n",
    "    np.linspace(movement[\"z_acc\"].min(), movement[\"z_acc\"].max(), 1000),\n",
    "    np.linspace(movement[\"y_acc\"].min(), movement[\"y_acc\"].max(), 1000),\n",
    ")\n",
    "\n",
    "# Note the correct column order: ['z_acc', 'y_acc']\n",
    "grid_df = pd.DataFrame(np.c_[X_1_grid.ravel(), X_2_grid.ravel()], columns=[\"z_acc\", \"y_acc\"])\n",
    "\n",
    "# Predict probabilities over the grid\n",
    "pred_prob = log_model.predict_proba(grid_df)\n",
    "\n",
    "# Select points near decision boundary (probability ≈ 0.5)\n",
    "mask = (pred_prob[:, 1] >= 0.49) & (pred_prob[:, 1] <= 0.51)\n",
    "sel_X_1 = X_1_grid.ravel()[mask]\n",
    "sel_X_2 = X_2_grid.ravel()[mask]\n",
    "sel_line = pred_prob[:, 1][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sigmoid surface and decision boundary\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\", computed_zorder=False)\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\", computed_zorder=False)\n",
    "\n",
    "axes = [ax1, ax2]\n",
    "for ax in axes:\n",
    "    surf = ax.plot_surface(\n",
    "        X_1_grid,\n",
    "        X_2_grid,\n",
    "        pred_prob[:, 1].reshape(X_1_grid.shape),\n",
    "        cmap=\"cool\",\n",
    "        antialiased=True,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        movement[\"z_acc\"], movement[\"y_acc\"], movement[\"move_type\"], c=movement[\"move_type\"], cmap=\"Set1\", alpha=0.8\n",
    "    )\n",
    "\n",
    "    ax.scatter(sel_X_1, sel_X_2, sel_line, c=\"black\", label=\"~0.5 prob\")\n",
    "\n",
    "ax1.set_xlabel(\"z_acc\", fontsize=12)\n",
    "ax1.set_ylabel(\"y_acc\", fontsize=12)\n",
    "ax1.set_zlabel(\"Predicted probability\", fontsize=12)\n",
    "\n",
    "ax1.view_init(elev=30.0, azim=70)\n",
    "ax2.view_init(elev=90.0, azim=-90)\n",
    "ax2.set_proj_type(\"ortho\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the top-down view (right plot), the sigmoid surface collapses into a **linear decision boundary**. This highlights how combining two features improves classification: the boundary is now more aligned with the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Logistic regression aims to separate classes by estimating probabilities and fitting a sigmoid function. However, it doesn’t explicitly maximize the distance between classes.\n",
    "\n",
    "Support Vector Machines (SVMs), on the other hand, aim to find the **decision boundary that maximizes the margin** between the classes. Only the data points closest to the boundary—called **support vectors**—influence this decision.\n",
    "\n",
    "Let’s train an SVM with a linear kernel on the same two features: `z_acc` and `y_acc`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Define and fit SVM model with a linear kernel\n",
    "clf = svm.SVC(kernel=\"linear\", C=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the decision boundary learned by the SVM, including:\n",
    "- The boundary itself (solid line)\n",
    "- The margin (dashed lines)\n",
    "- The support vectors (highlighted as large empty circles)\n",
    "\n",
    "The decision function tells us how far each point is from the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare grid again for contour plot\n",
    "xx = np.linspace(movement[\"z_acc\"].min(), movement[\"z_acc\"].max(), 1000)\n",
    "yy = np.linspace(movement[\"y_acc\"].min(), movement[\"y_acc\"].max(), 1000)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# Compute distance to decision boundary\n",
    "Z = clf.decision_function(pd.DataFrame(xy, columns=[\"z_acc\", \"y_acc\"])).reshape(XX.shape)\n",
    "\n",
    "# Create a new 2D plot (not 3D!)\n",
    "fig, ax2d = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot data and decision boundary\n",
    "sns.scatterplot(data=movement, x=\"z_acc\", y=\"y_acc\", hue=\"move_type\", palette=\"Set1\", ax=ax2d)\n",
    "\n",
    "# Plot the decision boundary and margins\n",
    "CS = ax2d.contour(XX, YY, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"])\n",
    "\n",
    "# Highlight support vectors\n",
    "ax2d.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    label=\"support\",\n",
    ")\n",
    "\n",
    "ax2d.clabel(CS, inline=1, fontsize=12)\n",
    "ax2d.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear SVMs with Kernels\n",
    "\n",
    "Sometimes the data cannot be separated with a straight line (or a plane in higher dimensions). This is where **kernel methods** come in.\n",
    "\n",
    "Support Vector Machines allow for **non-linear decision boundaries** by implicitly mapping the input features into a higher-dimensional space using a **kernel function**. One common example is the **polynomial kernel**.\n",
    "\n",
    "Let’s try this on the same dataset using a polynomial kernel of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train SVM with a non-linear (polynomial) kernel\n",
    "clf_nonlin = svm.SVC(kernel=\"poly\", degree=2, C=10)\n",
    "clf_nonlin.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the non-linear decision boundary and margins. Compared to the linear model, this approach can better handle curved class boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse same mesh grid\n",
    "xx = np.linspace(movement[\"z_acc\"].min(), movement[\"z_acc\"].max(), 1000)\n",
    "yy = np.linspace(movement[\"y_acc\"].min(), movement[\"y_acc\"].max(), 1000)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# Compute decision function values\n",
    "Z_nonlin = clf_nonlin.decision_function(pd.DataFrame(xy, columns=[\"z_acc\", \"y_acc\"])).reshape(XX.shape)\n",
    "\n",
    "# Plot\n",
    "fig, ax2d_nonlin = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.scatterplot(data=movement, x=\"z_acc\", y=\"y_acc\", hue=\"move_type\", palette=\"Set1\", ax=ax2d_nonlin)\n",
    "\n",
    "CS = ax2d_nonlin.contour(XX, YY, Z_nonlin, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"])\n",
    "\n",
    "ax2d_nonlin.scatter(\n",
    "    clf_nonlin.support_vectors_[:, 0],\n",
    "    clf_nonlin.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    label=\"support\",\n",
    ")\n",
    "\n",
    "ax2d_nonlin.clabel(CS, inline=1, fontsize=12)\n",
    "ax2d_nonlin.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "Unlike models that learn decision boundaries during training, **K-Nearest Neighbors (KNN)** makes decisions at prediction time. \n",
    "\n",
    "For any new data point, the algorithm:\n",
    "1. Finds the $k$ closest points in the training set,\n",
    "2. Assigns the most common class label among those neighbors.\n",
    "\n",
    "This is a **non-parametric**, instance-based method. Here, we use the default setting of $k=5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# Train KNN classifier\n",
    "kn_model = neighbors.KNeighborsClassifier()\n",
    "kn_model.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the decision regions predicted by KNN. You'll notice that the decision boundary may look more irregular, especially when $k$ is small or outliers are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels across the grid\n",
    "Z_kn = kn_model.predict(pd.DataFrame(xy, columns=[\"z_acc\", \"y_acc\"])).reshape(XX.shape)\n",
    "\n",
    "# Plot KNN decision boundary\n",
    "fig, ax_knn = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Fill contour for classification regions\n",
    "CS = ax_knn.contourf(XX, YY, Z_kn, cmap=\"Set2\")\n",
    "\n",
    "# Plot data points with both hue and style\n",
    "sns.scatterplot(\n",
    "    data=movement, x=\"z_acc\", y=\"y_acc\", hue=\"move_type\", style=\"move_type\", s=100, ax=ax_knn, palette=\"Set1\"\n",
    ")\n",
    "\n",
    "ax_knn.set_title(\"KNN Decision Regions (k=5)\")\n",
    "ax_knn.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Observation:** With KNN, the decision boundary can become sensitive to noise or outliers in the dataset. Choosing the right value of $k$ is essential for balancing bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Seed Classification\n",
    "\n",
    "We’ll now apply what we've learned to a new dataset: the **Seeds dataset** from the UCI Machine Learning Repository. This dataset contains measurements of various wheat seeds and their corresponding types.\n",
    "\n",
    "We'll focus on classifying two of the three seed types (classes 1 and 3).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Load and Preprocess the Data\n",
    "\n",
    "Load the dataset and keep only the two desired classes. This starting step is already provided for you. Make sure that you understand how the dataframe is filtered.\n",
    "\n",
    "Then, split your dataset into a train (80%) and test (20%) split. \n",
    "\n",
    "💡 Tip: Use `train_test_split` from `sklearn.model_selection`. You can split an entire dataframe without selecting specific columns first.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Logistic Regression\n",
    "\n",
    "Use logistic regression to classify the seeds based on:\n",
    "- First only `length_groove`\n",
    "- Then both `length_groove` and `perimeter`\n",
    "\n",
    "For each case:\n",
    "- Fit the model on the train set and make predictions on the test set\n",
    "- Evaluate the model using a classification report\n",
    "\n",
    "Which model performs better?\n",
    "\n",
    "💡 Tip: Use `classification_report` from `sklearn.metrics`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. K-Nearest Neighbors\n",
    "\n",
    "Train a KNN classifier on the same features. Compare the performance to logistic regression in terms of f1-score. Try different values for k (e.g., 3, 5, 7) and observe how the results change.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to experiment and visualize your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "seeds = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\",\n",
    "    sep=\"\\t\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    names=[\"area\", \"perimeter\", \"compactness\", \"length\", \"width\", \"symmetry_coef\", \"length_groove\", \"seed_type\"],\n",
    ")\n",
    "\n",
    "# Keep only seed types 1 and 3\n",
    "seeds = seeds[(seeds.seed_type == 1) | (seeds.seed_type == 3)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
