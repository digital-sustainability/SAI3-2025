{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Linear Regression to Non-Linear Models\n",
    "\n",
    "In the previous chapters, we explored how to use linear regression to model relationships in data — that is, when the target variable can be described as a weighted sum of input features:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + \\dots\n",
    "$$\n",
    "\n",
    "However, in many real-world scenarios, the relationship between variables is not linear. For example, in a parabolic relationship, the output depends on the square of an input:\n",
    "\n",
    "$$\n",
    "y = a x^2 + b\n",
    "$$\n",
    "\n",
    "The good news is: we can **extend linear regression to handle such non-linear patterns** by transforming our input features. This process — often called *feature engineering* — allows us to use standard linear regression techniques on transformed (non-linear) features.\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "- How to visualize non-linear relationships in data  \n",
    "- How to manually and automatically create non-linear features  \n",
    "- How to use polynomial regression with `scikit-learn`  \n",
    "- How to evaluate model performance and detect **underfitting** and **overfitting**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UCI wheat seeds dataset and display the first few rows\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from the UCI repository\n",
    "seeds = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt',\n",
    "    sep='\\t',\n",
    "    on_bad_lines='skip',\n",
    "    names=[\n",
    "        'area', 'perimeter', 'compactness', 'length', 'width',\n",
    "        'symmetry_coef', 'length_groove', 'seed_type'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "seeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between width and length of seeds\n",
    "\n",
    "sns.scatterplot(data=seeds, x=\"width\", y=\"length\")\n",
    "plt.title(\"Scatterplot of Seed Width vs. Length\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear regression model predicting length from width\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X=seeds[[\"width\"]], y=seeds[\"length\"])\n",
    "\n",
    "# Create prediction line over a range of width values\n",
    "x_range = np.arange(2.5, 4.1, 0.1).reshape(-1, 1)\n",
    "y_pred = model.predict(x_range)\n",
    "\n",
    "# Plot data and regression line\n",
    "sns.scatterplot(data=seeds, x=\"width\", y=\"length\")\n",
    "sns.lineplot(x=x_range.flatten(), y=y_pred, color=\"red\", label=\"Linear Fit\")\n",
    "plt.title(\"Linear Regression: Length ~ Width\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might guess a formula that involves also a quadratic term: $y = 6.7 - 2\\cdot \\text{width} + 0.5 \\cdot \\text{width}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually plot a parabolic curve to show a possible non-linear trend\n",
    "\n",
    "x_range = np.arange(2.5, 4.1, 0.1)\n",
    "y_parabola = 6.7 - 2 * x_range + 0.5 * x_range**2  # Just a guessed quadratic formula\n",
    "\n",
    "sns.scatterplot(data=seeds, x=\"width\", y=\"length\")\n",
    "sns.lineplot(x=x_range, y=y_parabola, color=\"green\", label=\"Manual Quadratic Curve\")\n",
    "plt.title(\"Illustrating a Non-Linear Relationship\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Linear Features into Non-Linear Ones: Feature Engineering\n",
    "\n",
    "What did we just do in the previous cell?\n",
    "\n",
    "We manually created a curve using a combination of terms like $x$ and $x^2$.  \n",
    "This gives us an important idea: we can **augment our dataset** by adding new features that are **non-linear transformations** of existing ones.\n",
    "\n",
    "Instead of modeling:\n",
    "$$\n",
    "y = w_0 + w_1 \\cdot x_1\n",
    "$$\n",
    "\n",
    "we now model:\n",
    "$$\n",
    "y = w_0 + w_1 \\cdot x_1 + w_2 \\cdot \\underbrace{x_1^2}_{:=x_2}\n",
    "$$\n",
    "\n",
    "This is still a linear regression model — but with **non-linear features**.\n",
    "\n",
    "We’ll now add a new column `width²` to our dataset and use it alongside `width` to fit a better model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a non-linear (squared) feature manually\n",
    "\n",
    "seeds[\"width2\"] = seeds[\"width\"] ** 2\n",
    "\n",
    "# Define input features and target variable\n",
    "X = seeds[[\"width\", \"width2\"]]\n",
    "y = seeds[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model on the original and squared width features\n",
    "\n",
    "model_nonlin = linear_model.LinearRegression()\n",
    "model_nonlin.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction inputs: both width and width^2 over a range\n",
    "x_range = np.arange(2.5, 4.1, 0.1)\n",
    "X_pred = np.stack([x_range, x_range**2], axis=1)\n",
    "\n",
    "# Plot the data and both models (linear and degree-2)\n",
    "sns.scatterplot(data=seeds, x=\"width\", y=\"length\")\n",
    "sns.lineplot(x=x_range, y=model.predict(x_range[:, np.newaxis]), color=\"red\", label=\"Linear\")\n",
    "sns.lineplot(x=x_range, y=model_nonlin.predict(X_pred), color=\"green\", label=\"Quadratic (degree 2)\")\n",
    "\n",
    "plt.title(\"Comparing Linear and Quadratic Fits\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need `np.stack` here?\n",
    "\n",
    "Our non-linear regression model was trained with **two input features**: `width` and `width²`.  \n",
    "So when we want to make predictions, we need to provide both of these features for each input value.\n",
    "\n",
    "We start with a vector of width values:\n",
    "\n",
    "```python\n",
    "x_range = np.arange(2.5, 4.1, 0.1)  # shape: (n_samples,)\n",
    "```\n",
    "\n",
    "Now we want to create a 2D array where:\n",
    "\n",
    "- The **first column** is `x_range` (the width values),\n",
    "- The **second column** is `x_range**2` (the squared values).\n",
    "\n",
    "We use `np.stack` to combine them **column-wise**:\n",
    "\n",
    "```python\n",
    "X_pred = np.stack([x_range, x_range**2], axis=1)\n",
    "```\n",
    "\n",
    "Now `X_pred` has shape `(n_samples, 2)`, which matches what the model expects:  \n",
    "a matrix where each row is an input `[width, width²]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance using Mean Squared Error (MSE)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "mse_linear = metrics.mean_squared_error(seeds[\"length\"], model.predict(seeds[[\"width\"]]))\n",
    "mse_quadratic = metrics.mean_squared_error(seeds[\"length\"], model_nonlin.predict(seeds[[\"width\", \"width2\"]]))\n",
    "\n",
    "print(f\"MSE (Linear Model):    {mse_linear:.5f}\")\n",
    "print(f\"MSE (Quadratic Model): {mse_quadratic:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Feature Expansion with `PolynomialFeatures`\n",
    "\n",
    "Instead of manually adding new features like $x^2$, we can use `scikit-learn`’s `PolynomialFeatures` class to do this automatically.\n",
    "\n",
    "This tool generates all polynomial combinations of the input features up to a specified degree.\n",
    "\n",
    "For example, if we input just `width` and set `degree=2`, it will generate:\n",
    "\n",
    "- a bias term (1),\n",
    "- `width`,\n",
    "- `width²`.\n",
    "\n",
    "We’ll now redo our previous example using this more general and scalable approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically generate polynomial features up to degree 2\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Instantiate the transformer for degree 2 polynomials\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Input feature (just 'width')\n",
    "X = seeds[[\"width\"]]\n",
    "\n",
    "# Transform into [1, width, width^2]\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how X looks like\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the transformed features X_poly\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model on the polynomial features (degree 2)\n",
    "\n",
    "model_poly = linear_model.LinearRegression()\n",
    "model_poly.fit(X=X_poly, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using the fitted polynomial model\n",
    "\n",
    "# Create width values and expand them into polynomial features\n",
    "x_range = np.arange(2.5, 4.1, 0.1)\n",
    "X_pred = poly.transform(x_range[:, np.newaxis])\n",
    "\n",
    "# Plot the original data and both model fits\n",
    "sns.scatterplot(data=seeds, x=\"width\", y=\"length\")\n",
    "sns.lineplot(x=x_range, y=model.predict(x_range[:, np.newaxis]), color=\"red\", label=\"Linear\")\n",
    "sns.lineplot(x=x_range, y=model_poly.predict(X_pred), color=\"green\", label=\"Degree 2 (PolynomialFeatures)\")\n",
    "plt.title(\"PolynomialFeatures vs. Linear Regression\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a polynomial regression model of degree 4\n",
    "\n",
    "# Prepare features and labels\n",
    "X = seeds[[\"width\"]]\n",
    "y = seeds[\"length\"]\n",
    "\n",
    "# Create polynomial features up to degree 4\n",
    "poly4 = PolynomialFeatures(degree=4)\n",
    "X_poly4 = poly4.fit_transform(X)\n",
    "\n",
    "# Fit the model\n",
    "model_poly4 = linear_model.LinearRegression()\n",
    "model_poly4.fit(X=X_poly4, y=y)\n",
    "\n",
    "# Generate predictions\n",
    "x_range = np.arange(2.5, 4.1, 0.1)\n",
    "X_pred4 = poly4.transform(x_range[:, np.newaxis])\n",
    "\n",
    "# Plot original data and all three fits\n",
    "sns.scatterplot(data=seeds, x=\"width\", y=\"length\")\n",
    "sns.lineplot(x=x_range, y=model.predict(x_range[:, np.newaxis]), color=\"red\", label=\"Linear\")\n",
    "sns.lineplot(x=x_range, y=model_poly.predict(poly.transform(x_range[:, np.newaxis])), color=\"green\", label=\"Degree 2\")\n",
    "sns.lineplot(x=x_range, y=model_poly4.predict(X_pred4), color=\"purple\", label=\"Degree 4\")\n",
    "plt.title(\"Comparing Linear, Degree 2, and Degree 4 Fits\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate underfitting and overfitting by using a small sample\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Take a small random sample of the data\n",
    "seeds_sample = seeds.sample(20, random_state=42)\n",
    "X_sample = seeds_sample[[\"width\"]]\n",
    "y_sample = seeds_sample[\"length\"]\n",
    "\n",
    "# Plot the data and fit polynomials of degrees 1, 2, and 8\n",
    "ax = sns.scatterplot(data=seeds_sample, x=\"width\", y=\"length\")\n",
    "models = {}\n",
    "\n",
    "for degree in [1, 2, 8]:\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X_sample)\n",
    "\n",
    "    model_poly = linear_model.LinearRegression()\n",
    "    model_poly.fit(X=X_poly, y=y_sample)\n",
    "\n",
    "    models[degree] = model_poly\n",
    "\n",
    "    # Generate predictions for plotting\n",
    "    x_range = np.arange(2.5, 4.3, 0.1)\n",
    "    X_pred = poly.transform(x_range[:, np.newaxis])\n",
    "    y_pred = model_poly.predict(X_pred)\n",
    "\n",
    "    # Print training error\n",
    "    mse = mean_squared_error(y_sample, model_poly.predict(X_poly))\n",
    "    print(f\"MSE (degree {degree}): {mse:.5f}\")\n",
    "\n",
    "    # Plot the model\n",
    "    sns.lineplot(x=x_range, y=y_pred, label=f\"Degree {degree}\")\n",
    "\n",
    "ax.set_title(\"Fitting on a Small Sample\")\n",
    "ax.set_ylim((3, 7))\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate models on a different data sample to assess generalization\n",
    "\n",
    "# Take a new random sample (not overlapping with the first one)\n",
    "seeds_sample2 = seeds.sample(20, random_state=22)\n",
    "X_val = seeds_sample2[[\"width\"]]\n",
    "y_val = seeds_sample2[\"length\"]\n",
    "\n",
    "# Predict using all three models on the new sample\n",
    "y_pred_1 = models[1].predict(PolynomialFeatures(degree=1).fit_transform(X_val))\n",
    "y_pred_2 = models[2].predict(PolynomialFeatures(degree=2).fit_transform(X_val))\n",
    "y_pred_8 = models[8].predict(PolynomialFeatures(degree=8).fit_transform(X_val))\n",
    "\n",
    "# Plot residuals (difference between actual and predicted values)\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(y_val.values - y_pred_1, label=\"Degree 1\")\n",
    "plt.plot(y_val.values - y_pred_2, label=\"Degree 2\")\n",
    "plt.plot(y_val.values - y_pred_8, label=\"Degree 8\")\n",
    "ax.set_title(\"Residuals on Validation Set\")\n",
    "ax.set_ylabel(\"Prediction Error\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print validation MSE for each model\n",
    "print(f\"MSE (degree 1): {mean_squared_error(y_val, y_pred_1):.5f}\")\n",
    "print(f\"MSE (degree 2): {mean_squared_error(y_val, y_pred_2):.5f}\")\n",
    "print(f\"MSE (degree 8): {mean_squared_error(y_val, y_pred_8):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate validation error across a range of polynomial degrees\n",
    "\n",
    "# Define training and validation samples\n",
    "seeds_sample = seeds.sample(20, random_state=42)\n",
    "X_train = seeds_sample[[\"width\"]]\n",
    "y_train = seeds_sample[\"length\"]\n",
    "\n",
    "seeds_sample2 = seeds.sample(20, random_state=22)\n",
    "X_val = seeds_sample2[[\"width\"]]\n",
    "y_val = seeds_sample2[\"length\"]\n",
    "\n",
    "# Evaluate models of increasing complexity (degrees 0 to 9)\n",
    "errors = []\n",
    "\n",
    "for degree in range(10):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    model_poly = linear_model.LinearRegression()\n",
    "    model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "    val_pred = model_poly.predict(X_val_poly)\n",
    "    mse = mean_squared_error(y_val, val_pred)\n",
    "    errors.append(mse)\n",
    "\n",
    "# Plot validation error by model degree\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(10), errors, \"-o\")\n",
    "ax.set_title(\"Validation Error vs. Polynomial Degree\")\n",
    "ax.set_xlabel(\"Polynomial Degree\")\n",
    "ax.set_ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Splits with `train_test_split`\n",
    "\n",
    "In practice, we rarely sample training and validation data manually.\n",
    "\n",
    "Instead, we use utility functions like `train_test_split` from `scikit-learn` to randomly split our dataset into **training** and **testing (validation)** sets.\n",
    "\n",
    "This allows us to:\n",
    "\n",
    "- Train our model on one portion of the data\n",
    "- Evaluate how well it generalizes on unseen data\n",
    "- Avoid overfitting by tuning model complexity based on validation performance\n",
    "\n",
    "Let’s see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically split the dataset into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split: 70% training, 30% test\n",
    "seeds_train, seeds_test = train_test_split(seeds, test_size=0.3, random_state=0)\n",
    "\n",
    "print(f\"Training samples: {len(seeds_train)}\")\n",
    "print(f\"Testing samples:  {len(seeds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "These exercises will help you practice what you’ve learned using a new dataset.\n",
    "\n",
    "\n",
    "### Dataset: `centripetal.csv`\n",
    "\n",
    "This dataset was collected using a smartphone while someone rotated in place. It includes the following columns:\n",
    "\n",
    "- `Angular velocity (rad/s)`: the rotational speed in **radians per second**  \n",
    "- `Acceleration (m/s^2)`: the **inward acceleration** experienced during rotation (as on a carousel), measured in **m/s²**\n",
    "\n",
    "There is a **non-linear relationship** between these two quantities:\n",
    "$$\n",
    "a = \\omega^2 \\cdot r\n",
    "$$\n",
    "where $a$ is the centripetal acceleration, $\\omega$ is the angular speed, and $r$ is the radius (assumed constant here).\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Load and explore the data**  \n",
    "   - Load the `centripetal.csv` file.  \n",
    "   - Plot centripetal acceleration vs. angular speed.\n",
    "\n",
    "2. **Fit a linear regression model**  \n",
    "   - Use acceleration to predict angular speed.  \n",
    "   - Compute and report the **mean squared error (MSE)**.\n",
    "\n",
    "3. **Fit a polynomial regression model (degree 2)**  \n",
    "   - Use `PolynomialFeatures` to generate features.  \n",
    "   - Fit a model and compute the MSE.  \n",
    "   - Which model fits the data better?\n",
    "\n",
    "4. **Test generalization with train/test split**  \n",
    "   - Use `train_test_split` to create an 80% training / 20% test split.  \n",
    "   - Fit a **degree 10 polynomial model** on the training set.  \n",
    "   - Plot the model’s predictions and evaluate its test error.  \n",
    "   - What do you observe? Why might this happen?\n",
    "\n",
    "\n",
    "💡 **Hint:** Centripetal acceleration increases **non-linearly** with angular speed. This is a great example of a case where a **linear model may be misleading**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
