{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjwMVdVbkr0P"
   },
   "source": [
    "# Combining and grouping DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mM-peDakr0T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAUEMdaLkr0U"
   },
   "source": [
    "Two of the most important operations one can do with DataFrames is 1) combine multiple sources of information and 2) efficiently group and summarize information. Both of these topics are vast and here again, we ony present a few important approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co9Kh2lakr0U"
   },
   "source": [
    "## Concatenation\n",
    "\n",
    "The simplest case where we need to combine multiple sources of information is if those sources are of the same \"type\", i.e. they have the same columns but different entries. Imagine for example lists of participants to an event broken in muliple files with names starting with A-K, L-S etc. In that case we can just \"glue\" datasets together or more formally **concatenate** them. Here we load to Excel sheets with composer information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1548,
     "status": "ok",
     "timestamp": 1614355561199,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "wnMcwQe7kr0U",
    "outputId": "5e3b58ad-7265-4b5f-b5e1-fe1391f55763"
   },
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/guiwitz/NumpyPandas_course/blob/master/Data/composers.xlsx?raw=true'\n",
    "composers1 = pd.read_excel(file_url, index_col='composer',sheet_name='Sheet1')\n",
    "composers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1614355561687,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "NOUxMkx4kr0V",
    "outputId": "8b06afc5-6ce7-4903-e393-56a3dd8a17de"
   },
   "outputs": [],
   "source": [
    "composers2 = pd.read_excel('https://github.com/guiwitz/NumpyPandas_course/blob/master/Data/composers.xlsx?raw=true', index_col='composer',sheet_name='Sheet3')\n",
    "composers2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zag-3SYukr0W"
   },
   "source": [
    "We see that we have the same information in both tables, but for different composers. We can just concetenate them using the ```pd.concat``` function and providing the two DataFrames in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDoMbe-ekr0W"
   },
   "outputs": [],
   "source": [
    "all_composers = pd.concat([composers1,composers2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 2141,
     "status": "ok",
     "timestamp": 1614355623399,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "dhr9Mnqkkr0W",
    "outputId": "b3563973-9c41-4755-acbc-fa9c19e4b421"
   },
   "outputs": [],
   "source": [
    "all_composers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sa-zZEs2kr0W"
   },
   "source": [
    "One potential problem is that two tables contain duplicated information, like here the Mahler entry. Pandas offers a lot of useful functions to clean-up data, for example ```drop_duplicates()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1614355632075,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "LuHWdzRouqv0",
    "outputId": "67f247f2-0bdd-435d-9cf1-2f2275182c52"
   },
   "outputs": [],
   "source": [
    "all_composers.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCNQd-DTkr0Y"
   },
   "source": [
    "## Joining two tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHbJD2FWkr0Y"
   },
   "source": [
    "A slightly more complex case is if we have two datasets with (almost) the same items, in our case composers, but with different information that we want to combine. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1614355658781,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "lKODXCAOkr0Y",
    "outputId": "a46530b4-d3ff-409e-c06d-5bd3821ef354"
   },
   "outputs": [],
   "source": [
    "composers1 = pd.read_excel(file_url, index_col='composer',sheet_name='Sheet1')\n",
    "composers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1614355660084,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "vnbi4f3Pkr0Y",
    "outputId": "58ed7870-59e9-4db1-9960-b694d9aa5560"
   },
   "outputs": [],
   "source": [
    "composers2 = pd.read_excel(file_url, index_col='composer',sheet_name='Sheet4')\n",
    "composers2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcxqfLPwkr0Y"
   },
   "source": [
    "We cannot concatenate the tables as they have different columns. However we can go through both tables and **merge for each item the information of both tables**. Note that each table has **one** element that doesn't appear in the other table (Shostakovitch and Brahms).\n",
    "\n",
    "We can do this merging operation with the ```join``` function, which one uses like this ```left_table.join(right_table)```: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1614355699842,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "I-IsWWFxkr0Z",
    "outputId": "25d77a1b-0025-42ee-eed6-767213672c9e"
   },
   "outputs": [],
   "source": [
    "composers1.join(composers2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxt8LU4Kkr0a"
   },
   "source": [
    "We see that:\n",
    "1. Pandas used the index to merge all elements\n",
    "2. Brahms which was missing in ```composers1``` is absent\n",
    "3. The first name of Shostakovitch is missing since that composer was absent in ```composers2```.\n",
    "\n",
    "In other terms, ```join``` used all the elements of the ```left``` table and only those matching from the ```right``` table. This is just a default that can be overriden using the ```how``` option.\n",
    "\n",
    "In total we have four possibilities as illustrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1614531167787,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "zDe-n9GXjiG3",
    "outputId": "98f43869-5ad1-4409-e824-39d8c73b3d89"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://github.com/guiwitz/ISDAwPython_day2/raw/master/images/left_right.jpeg',width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txardHSHjkJO"
   },
   "source": [
    "1. keep all elements of the ```rigth``` table and complete with the matching ones from the ```left``` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1614355732809,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "MprjZbzZkr0b",
    "outputId": "8f173252-67ce-4fbf-bd9f-772f63d7712f"
   },
   "outputs": [],
   "source": [
    "composers1.join(composers2, how = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwrHxbp3xuPh"
   },
   "source": [
    "2. Keep all elements of both tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1614355736661,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "_9kWG9SWkr0c",
    "outputId": "be2a2725-24dc-409a-9a8d-69d122058bfa"
   },
   "outputs": [],
   "source": [
    "composers1.join(composers2, how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArqBxRgSyHrl"
   },
   "source": [
    "3. Keep **only** elements present in **both tables**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1614355739788,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "-E5_z4Idkr0c",
    "outputId": "daf26cf9-a6b3-4ecd-84b8-a96fa4bd1427"
   },
   "outputs": [],
   "source": [
    "composers1.join(composers2, how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gcWC82rkr0c"
   },
   "source": [
    "## More general approach\n",
    "\n",
    "Here we used the index to know which items to combined together, but more generally one can use **any** column for merging. The ```pd.merge``` function offers more flexibility in this regard. We whos only one example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_1PE2ANkr0d"
   },
   "outputs": [],
   "source": [
    "composers1 = pd.read_excel(file_url, sheet_name='Sheet1')\n",
    "composers2 = pd.read_excel(file_url, sheet_name='Sheet6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1614355747801,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "lN-GpM95kr0d",
    "outputId": "9489f158-966d-45d7-da80-1847c67d97e4"
   },
   "outputs": [],
   "source": [
    "composers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1614355749559,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "03-VQTJmkr0d",
    "outputId": "3a926031-a238-4042-b903-b0552987b30f"
   },
   "outputs": [],
   "source": [
    "composers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1614355762515,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "2-_TN1rGkr0d",
    "outputId": "df150c99-b0b3-49c7-e356-8bd076513c8e"
   },
   "outputs": [],
   "source": [
    "pd.merge(composers1, composers2, left_on='composer', right_on='last name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paXowyLGzBeE"
   },
   "source": [
    "The concepts are the same as in the ```join()``` function. The first table (```composers1```) is the ```left``` table, the second one (```composers2```) is the ```right``` table. And then we specify in ```left_on``` and ```right_on``` which columns to use for the merging. Finally, the ```how```option is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sld_Uwk_2Pc"
   },
   "source": [
    "## Grouping\n",
    "\n",
    "Very often when you want to calculate some statistics on a dataset you need to group some of the data. Let's look at our penguin dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7qclqKVAibe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "penguins = pd.read_csv('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1614454936929,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "360sD8XQAwK7",
    "outputId": "4faf0e50-5894-41ab-dc43-2d37910f6911"
   },
   "outputs": [],
   "source": [
    "penguins.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1614454947104,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "-sdCTkOqA1vE",
    "outputId": "d3942ef9-78ce-47ab-ed61-1b8a970b8e01"
   },
   "outputs": [],
   "source": [
    "penguins.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IL1_PTCA4Hc"
   },
   "source": [
    "We see that we have data for three different species of Penguins. What if we want to have the average weight of penguins by species ? One solution would be to make a for loop e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmVe67dSBF9h"
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "for species in penguins.species.unique():\n",
    "  mean_w = penguins.loc[penguins['species'] == species,'body_mass_g'].mean()\n",
    "  weights.append(mean_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1614455133162,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "BH2vkmsLBkgc",
    "outputId": "02d44557-e42a-4c66-9b12-ca00fbc18212"
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkTqIgE0Bldd"
   },
   "source": [
    "Pandas offers a much simpler solution for this kind of operation with the ```.groupby``` function. Here we only specifiy which columns should be used to form groups and it does the work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL4XrwfHB3N2"
   },
   "outputs": [],
   "source": [
    "grouped_penguins = penguins.groupby('species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1614455225637,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "H2Zc_7LiB64Q",
    "outputId": "faa62d51-2830-45ab-f46c-2350a09269f1"
   },
   "outputs": [],
   "source": [
    "grouped_penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WVHs0qmB79x"
   },
   "source": [
    "The output is not directly visible. It's a collection of tables grouped according to species. Many functions allow us to get one of these subgroups, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1614455280029,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "dpOuhFV5CF8F",
    "outputId": "99d914c1-009b-4dd2-86e2-7bdc6d398646"
   },
   "outputs": [],
   "source": [
    "grouped_penguins.get_group('Adelie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao4wCb6eCJut"
   },
   "source": [
    "However we wont go here through the details of the group data structure. What we can simply do is magically apply functions directly on the grouped object. For example to get the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1614455346981,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "2DYR5mgwCXkv",
    "outputId": "2647df41-d8f2-4793-9f7c-4bd32cf37bc5"
   },
   "outputs": [],
   "source": [
    "grouped_penguins.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU-8oVg-CZpG"
   },
   "source": [
    "As you can see Pandas automatically computes the mean for each category and each column. The output then is a DataFrame where each line corresponds to a given category. One can push this further by using e.g. multiple columns for grouping, but this goes beyond the present course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2ZRaOyyzYII"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Load a first DataFrame that contains information about covid19 vaccination locations: https://github.com/owid/covid-19-data/blob/master/public/data/vaccinations/locations.csv \n",
    "2. Load a second DataFrame which contains daily information about vaccination for every country: https://github.com/owid/covid-19-data/blob/master/public/data/vaccinations/vaccinations.csv\n",
    "3. Create a sub-dataframe of the table in (1) that only contains the fields ```location``` and ```vaccines```.\n",
    "4. Merge the two tables using the ```location``` field as key for merging. Use once ```left``` merge and once ```right``` marge.\n",
    "5. Do you see a difference in the result?\n",
    "5b. Bonus question: knowing that you can use the ```.unique()``` method on columns, and that there's a function called ```np.isin``` that compares two lists, can you figure out which country is missing ?\n",
    "6. Using the table from (1), group the data by ```location``` and calculate the average number of ```daily_vaccinations```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II0smv8RPQTc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "09-Pandas_combine.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/guiwitz/NumpyPandas_course/blob/colab/10-DA_Pandas_combine.ipynb",
     "timestamp": 1614296569460
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
