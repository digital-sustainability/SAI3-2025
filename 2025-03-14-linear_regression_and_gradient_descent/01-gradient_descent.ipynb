{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800f857b-5105-4fe0-8b57-9003086f4afe",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "This is a supplementary information notebook. None of this content needs to be reproduced for the course or the exam but questions could be asked about it during the exam.\n",
    "\n",
    "In the introduction to linear regression we have seen that for a simple case, we could compute the error made a by a series of models (each model is defined by its parameters). In that case, we can just figure out which model (and thus which parameters) have the lowest error. But in reality, we can rarely do that. We usually rely on searching the solution via gradient descent, i.e. we start from the green point and try to figure out where the minimum (the yellow star) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48965cd6-1380-4db7-ae73-aa9fb4418dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98b845-978d-43b2-b967-b6be831b85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify the different ranges that we want to explore\n",
    "w0_range = np.arange(-1000, 0, 5)\n",
    "w1_range = np.arange(0, 1000, 5)\n",
    "\n",
    "# 2. Create a grid of values for the parameters\n",
    "grid_0, grid_1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# 3. Predict the fixed acidity values for each combination of parameters (w0, w1)\n",
    "grid_mult = grid_0[:, :, np.newaxis] + grid_1[:, :, np.newaxis] * wine[\"density\"].values\n",
    "\n",
    "# 4. Compute the squared error for each combination of parameters\n",
    "grid_mult = (grid_mult - wine[\"fixed acidity\"].values) ** 2\n",
    "total_error = np.sum(grid_mult, axis=2)\n",
    "\n",
    "# 5. Find the minimal error\n",
    "min_loc = np.unravel_index(np.argmin(total_error), total_error.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "X = grid_0.copy()\n",
    "Y = grid_1.copy()\n",
    "Z = total_error.copy()\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1, projection=\"3d\", computed_zorder=False)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)  # , vmax=5)\n",
    "\n",
    "# Add green dot and yellow star to indicate start and goal\n",
    "ax.scatter([grid_0[180, 150]], [grid_1[180, 150]], [total_error[180, 150]], c=\"green\", marker=\"o\", s=50)\n",
    "ax.scatter(\n",
    "    [grid_0[min_loc[0], min_loc[1]]],\n",
    "    [grid_1[min_loc[0], min_loc[1]]],\n",
    "    [total_error[min_loc[0], min_loc[1]]],\n",
    "    c=\"yellow\",\n",
    "    marker=\"*\",\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$w_1$\", fontdict={\"fontsize\": 20})\n",
    "ax.set_ylabel(\"$w_0$\", fontdict={\"fontsize\": 20});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbfedc-0ac7-44c8-a1f9-92b00b086085",
   "metadata": {},
   "source": [
    "How do we now reach the bottom of the valley from this point? It's also important to remember that we don't *see* the whole surface. It's as if we were on a snowy mountain in the fog: we see a few meters from our current position and the only information we have is the direction of the slope.\n",
    "\n",
    "To simplify things, let's just look at a 1d example where our error as a function of a single parameter $w_0$ looks like a parabola: $e = x^2$ where $e$ is the error and $x$ the single parameter of the model. Here again we want to find the bottom of the valley. Let's also start from a random point (green), in this example we choose $w_0 = 7.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f4c45-fac3-47c3-ae5e-f65b3099edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabola(x):\n",
    "    return x**2\n",
    "\n",
    "w0_range = np.arange(-10, 10, 0.5)\n",
    "error = parabola(np.arange(-10, 10, 0.5))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error,'b^')\n",
    "ax.plot([-7.5], [parabola(-7.5)], 'go', label='start', markersize=10);\n",
    "ax.set(xlim=[-10,10], ylim=[-1,100], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297f56b-2138-4145-b999-33c5a9dfa64a",
   "metadata": {},
   "source": [
    "We can now slide along the slope until we find the bottom. Again remember that we don't \"see\" the whole curve, and therefore at this moment we only see our current position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f7059-e829-44f1-ad87-d338a1ec0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range = np.arange(-7.5, -7.5, 0.5)\n",
    "error = parabola(w0_range)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error, 'b^')\n",
    "ax.plot([-7.5], [parabola(-7.5)], 'go', markersize=10, label='start');\n",
    "ax.set(xlim=[-10,10], ylim=[-1,100], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b4c92-966c-49e7-9cb4-b975132fa051",
   "metadata": {},
   "source": [
    "What we can do is slightly vary the value of our current choice of $w_0$ and estimate the error. This would give us the following information. Let's create two points around $w_0=-7.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce730325-3433-4990-a1f1-06fda7c3bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range = np.arange(-8, -6.5, 0.5)\n",
    "error = parabola(np.arange(-8, -6.5, 0.5))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error, 'b^', label='surrounding')\n",
    "ax.plot([-7.5], [parabola(-7.5)], 'go', markersize=10, label='start');\n",
    "ax.set(xlim=[-10,10], ylim=[-1,100], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a076002-acf1-47ac-a8dc-eba2b983ad2c",
   "metadata": {},
   "source": [
    "We now have our current point, plus some information on the local surroundings. In particular we see now in which direction we should go to decrease the error: to the right, i.e. larger values of $w_0$.\n",
    "\n",
    "If we want to follow the slope, we can just estimate it by drawing a line between the close-by points and calculate the slope of that line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55212f3a-b79f-4100-956f-cc3128ef3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = (error[1]-error[0]) / (w0_range[1]-w0_range[0])\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883f7f5-c4db-42e4-842e-3acf83d150a6",
   "metadata": {},
   "source": [
    "Now we know that if we move to the right, i.e. use **larger $w_0$ values** (which corresponds to a **negative slope**), the **error will decrease**. If at some point the slope becomes positive, it means that we have to move to the left, i.e. make $w_0$ smaller, otherwise we are increasing the error again. Therefore, we can use the slope to update the value of $w_0$. We can say that $w_0^{\\texttt{new}} = w_0 - \\texttt{lr} \\cdot \\texttt{slope}$ with $\\texttt{lr}$ being a multiplicative factor called *learning rate*, e.g. 0.1. Thus we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e617a67-56a7-4c64-bf81-a35fe4343f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0_new = -7.5 - slope * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf873fd-f645-4ec8-8fb2-239d4c7148ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range = np.arange(-8, -6.5, 0.5)\n",
    "error = parabola(np.arange(-8, -6.5, 0.5))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error, 'b^', label='surrounding ($t_0$)')\n",
    "ax.plot([-7.5], [parabola(-7.5)], 'o', color='gray', markersize=10, label='timestep $t_0$');\n",
    "ax.plot([w_0_new], [parabola(w_0_new)], 'go', markersize=10, label='timestep $t_1$');\n",
    "ax.set(xlim=[-10,10], ylim=[-1,100], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4132a3f-ee46-47b1-893a-835316f352bf",
   "metadata": {},
   "source": [
    "We have moved in the right direction! Now we can repeat the operation. Compute surrounding points, the local slope and update the value of $w_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c8b07-466b-458a-a19b-caa92a68aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range2 = np.arange(w_0_new-0.5, w_0_new+0.6, 0.5)\n",
    "error2 = parabola(w0_range2)\n",
    "slope = (error2[-1]-error2[0]) / (w0_range2[-1]-w0_range2[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "previous_w_0 = w_0_new\n",
    "w_0_new = w_0_new - slope * 0.1\n",
    "\n",
    "ax.plot(w0_range, error, \"b^\", label=\"surrounding ($t_0$)\")\n",
    "ax.plot(w0_range2, error2, \"m^\", label=\"surrounding ($t_1$)\")\n",
    "\n",
    "ax.plot([-7.5], [parabola(-7.5)], \"o\", color=\"gray\", markersize=10, label=\"timestep $t_0$\")\n",
    "ax.plot([previous_w_0], [parabola(previous_w_0)], \"o\", color=\"gray\", markersize=10, label=\"timestep $t_1$\")\n",
    "ax.plot([w_0_new], [parabola(w_0_new)], \"go\", markersize=10, label=\"timestep $t_2$\")\n",
    "\n",
    "ax.set(xlim=[-10,10], ylim=[-1,100], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccbf0e3-a3f2-4d1d-89ed-4c1410a51bee",
   "metadata": {},
   "source": [
    "We can repeat now the operation until we reach the bottom of the valley. Notice that when we reach to bottom, the slope should be close to zero and therefore we should stop moving automatically! We can plot the entire series of updated values for $w_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7f00f-0182-4cce-8ecf-51fd8d2fea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "w_0_vals = [-7.5]\n",
    "error_vals = [parabola(w_0_vals[0])]\n",
    "\n",
    "for i in range(20):  \n",
    "    w0_range = np.arange(w_0_vals[-1]-0.5, w_0_vals[-1]+0.6, 0.5)\n",
    "    error = parabola(w0_range)\n",
    "    slope = (error[-1]-error[0]) / (w0_range[-1]-w0_range[0])\n",
    "\n",
    "    w_0_new = w_0_vals[-1] - slope * 0.1\n",
    "    w_0_vals.append(w_0_new)\n",
    "    error_vals.append(parabola(w_0_new))\n",
    "\n",
    "ax.plot(w_0_vals, error_vals, 'go--');\n",
    "\n",
    "ax.set(xlim=[-10,10], ylim=[-1,100], xlabel='$w_0$', ylabel='error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981bfaf-c42d-4387-bb84-aad142bd1197",
   "metadata": {},
   "source": [
    "We clearly reach the bottom and we also clearly see that when the slope decreases, the steps we take keep getting smaller.\n",
    "\n",
    "This type of method where we use the local slope to search for the minimum is called **gradient descent**. The gradient is a general term for slope in the case where we deal with more than one variable (like in the surface shown previously) and descent means going down. There are many ways to computer the slope, update the parameters etc. and we won't go into such details. \n",
    "\n",
    "The important idea here is that we use the local slope information to reach a global minimum. Most of the time you will hear about Stochastic Gradient Descent (SGD) which uses this approach but where only a fraction (a so called *batch*) of the data is used at each iteration. This is particularly relevant when using very large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d71fb5-6132-47b1-8d2c-b8a227ea8ba2",
   "metadata": {},
   "source": [
    "## Potential problems\n",
    "\n",
    "When using any type of Machine Learning algorithm, one has to know about its limitations and typical pitfalls. Some of them are common to many algorithms and some are method specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea862e-2513-49a1-8764-774721a09194",
   "metadata": {},
   "source": [
    "### Initial conditions\n",
    "\n",
    "When we executed the fit with the standard linear regression, we could just use ```res = model.fit(X=X, y=y)``` but when we used the gradient descent method above, we had to use\n",
    "```res = model.fit(X=X, y=y, coef_init=600, intercept_init=-600)```, i.e. we had to manually specify a starting point with ```coef_init``` and ```intercept_init```. This is because unlike the standard method, gradient descent actually *searches* for a solution, and if we start too far away, we might never find it. Let's try without:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248f861-7076-46dd-b025-6a4efcfd4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X = wine[['density']]\n",
    "y = wine['fixed acidity']\n",
    "\n",
    "model = SGDRegressor()\n",
    "res = model.fit(X=X, y=y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity');\n",
    "plt.plot(np.arange(0.99, 1.004, 0.001), res.intercept_ + np.arange(0.99, 1.004, 0.001)*res.coef_[0], 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2e9aa-dc00-4d63-81a8-33aa1feae279",
   "metadata": {},
   "source": [
    "Clearly gradient descent didn't find the correct solution. There are mainly two reasons that could explain this: we didn't do enough steps or the algorithm found another \"not too bad\" solution and couldn't escape it.\n",
    "\n",
    "We can try to adjust the number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ba031-82ef-4bd6-b7c3-45a525379211",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor(max_iter=10000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "res = model.fit(X=X, y=y)\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity');\n",
    "plt.plot(np.arange(0.99, 1.004, 0.001), res.intercept_ + np.arange(0.99, 1.004, 0.001)*res.coef_[0], 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250864b3-f6a5-486c-912d-8ffc1dbdf206",
   "metadata": {},
   "source": [
    "It doesn't seem to do much to solve our problem, so we are probably trapped in some local minimum. To understand this, let's imagine that our 1D erro plot looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cc40d-f663-412e-b88d-e310b865ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range = np.arange(-10, 10, 0.5)\n",
    "def error_fun(x):\n",
    "    return x**2 * np.abs(np.cos(4*x/20)) + np.abs(x)/1\n",
    "error = error_fun(w0_range)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error, 'b^')\n",
    "ax.plot([0], error_fun(np.array([0])), 'y*', markersize=12, label='true minimum');\n",
    "#ax.plot([w_0_new], [parabola(w_0_new)], 'go', label='new');\n",
    "ax.set(xlim=[-10,10], ylim=[-10,60], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336f936-be8d-41a6-9db6-6e4c9c7fa46e",
   "metadata": {},
   "source": [
    "If we start our search too far on the left or the right and follow the slope, we will end up in one of the local minima. For example, if we start at $w_0 = -6$ we will end up in the local minimum on the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fa4c5-262d-4a53-b015-252a4ac26d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error, 'b^')\n",
    "ax.plot([0], [0], \"y*\", markersize=12, label=\"true minimum\")\n",
    "ax.plot([-6], [error_fun(-6)], 'o', color=\"gray\", markersize=10, label='start');\n",
    "ax.plot([-8], [error_fun(-8)], 'go', markersize=10, label='end');\n",
    "ax.set(xlim=[-10,10], ylim=[-10,60], xlabel='$w_0$', ylabel='error')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1e637-8705-4c92-8dce-a2373a3e8d8d",
   "metadata": {},
   "source": [
    "So in this case we would have to specify an initial condition, e.g. $w_0=-5$. Note that there are alternatives to take care of this. For example one can run the optimization several times from different initial points and keep the *lowest* of the found minima, or we can add some noise at each step to help the optimizer get over small \"hills\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb360c10-a5ea-4be2-8fa9-21bcd878edd5",
   "metadata": {},
   "source": [
    "### Flat gradient\n",
    "\n",
    "As we use the gradient to reach the minimum, if the error curve is flat *somewhere else* than at the true minimum, it can start to wander randomly as there is no slope anymore to follow. This would be the case for example with an error like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450e739-3cee-4096-ae57-f6993e90b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range = np.arange(-10, 10, 0.5)\n",
    "def error_fun(x):\n",
    "    out = x**2\n",
    "    out[x > 5]=25\n",
    "    return out\n",
    "error = error_fun(w0_range)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_range, error, 'b^')\n",
    "ax.plot([0], error_fun(np.array([0])), 'y*', markersize=12, label='true minimum');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b4b35-4cbe-4772-a2f7-4d1119b7119b",
   "metadata": {},
   "source": [
    "If we start here around $w_0=7.5$ for example, the slope is zero and therefore the step to compute the new parameter value $w_0^{new} = w_0 - lr * slope$ is zero: we don't converge to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e9dca-d7ad-4ec5-a598-837b81642dc4",
   "metadata": {},
   "source": [
    "### Over-shooting solution\n",
    "\n",
    "The *learning rate* parameter in our weight update formula $w_0^{\\texttt{new}} = w_0 - \\texttt{lr} \\cdot \\texttt{slope}$ is very important in that it dictates how big we want the update/step to be. If it is too small, then it will take forever to find the solution. If it is too large, there's a risk that we miss the solution because we move too fast. For example if we use again our loop above but use a larger learning rate, then sequence of steps is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4095566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent(w0_range, w0_start, error_function, learning_rate=0.8, n_iter=20, print_steps=True):\n",
    "    \"\"\"\n",
    "    Plot the gradient descent steps for a given error function and learning rate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0_range : np.array\n",
    "        The range of w0 values to consider.\n",
    "    w0_start : float\n",
    "        The starting value for w0.\n",
    "    error_function : function\n",
    "        The error function to minimize.\n",
    "    learning_rate : float\n",
    "        The learning rate for the weight update.\n",
    "    n_iter : int\n",
    "        The number of iterations (gradient descent steps) to perform.\n",
    "    print_steps : bool\n",
    "        Whether to print information about each step.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    error = error_function(w0_range)\n",
    "    ax.plot(w0_range, error, 'b^')\n",
    "\n",
    "    w_0_vals = [w0_start]\n",
    "    error_vals = [error_function(w0_start)]\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        w0_range_local = np.arange(w_0_vals[-1] - 0.5, w_0_vals[-1] + 0.6, 0.5)\n",
    "        local_error = error_function(w0_range_local)\n",
    "        slope = (local_error[-1] - local_error[0]) / (w0_range_local[-1] - w0_range_local[0])\n",
    "\n",
    "        w_0_new = w_0_vals[-1] - slope * learning_rate\n",
    "        w_0_vals.append(w_0_new)\n",
    "        error_new = error_function(w_0_new)\n",
    "        error_vals.append(error_new)\n",
    "\n",
    "        print(f\"Step {i+1}: w0={w_0_new}, error={error_new}\")\n",
    "\n",
    "    ax.plot(w_0_vals, error_vals, \"go--\")\n",
    "    ax.set(xlim=[w0_range.min()-0.5, w0_range.max()+0.5], ylim=[error.min(), error.max()], xlabel=\"$w_0$\", ylabel=\"error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf13bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(\n",
    "    w0_range=np.arange(-10, 10, 0.5),\n",
    "    w0_start=-7.5,\n",
    "    error_function=parabola\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f580a4e-8e4c-4aef-9fbe-07e9c80bb20b",
   "metadata": {},
   "source": [
    "We see that we still converge to the final solution, but we hop from one side to the other of the error plot. In this toy example, we were lucky that the error function is just a parabola with one global minimum. If the error function is more complex, we might never find the global minimum. Below you can see an example of this when we assume that the error function has multiple local minima (the function that you have seen before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f18251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_fun(x):\n",
    "    return x**2 * np.abs(np.cos(4 * x / 20)) + np.abs(x) / 1\n",
    "\n",
    "plot_gradient_descent(\n",
    "    w0_range=np.arange(-10, 10, 0.2),\n",
    "    w0_start=-10,\n",
    "    error_function=error_fun,\n",
    "    learning_rate=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7538b-18c9-427d-a25c-dacbccfd35c5",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "We have seen only a tiny fraction of existing linear regression methods. However these simple examples have allowed us to learn fundamental concepts:\n",
    "- metrics: how do we measure the quality of our model. In this case we used the sum of squared errors\n",
    "- gradient descent: using the local slope to find the minimum in the error and therefore the best parameters\n",
    "- potential pitfalls like vanishing gradients (no slope), learning rate etc.\n",
    "\n",
    "Note that all these concept will reappear in other methods, in particular in the most advanced ones like Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d61f17",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the `plot_gradient_descent` function together with the previously defined `error_fun` error function and solve the following tasks:\n",
    "\n",
    "1. Set your range for the $w_0$ values in the plot from $-20$ to $20$. Then start at $w_0 = -12$ with a learning rate of $0.7$ and perform gradient descent. How many iterations do you need to find the minimum? Start with 10 iterations and then gradually increase until you found a number that works. You can juste execute the same cell multiple times with different `n_iter` values.\n",
    "2. Now in another cell, use the same arguments for `plot_gradient_descent`, but this time change the learning rate to $0.2$. You will see that you find the minimum much faster. How many steps do you need now? Try to decrease the steps until you don't find the minimum anymore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
