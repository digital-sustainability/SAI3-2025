{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64992822-fd35-45b6-b2d7-fa5fd063687e",
   "metadata": {},
   "source": [
    "# Basic ML concepts via Linear Regression\n",
    "\n",
    "The goal of regression is to define a model that explains one measurement using another (or multiple) measurement(s). For example we expect that on average the weight of people depends on their height. The simplest assumption is that a this relationship can be described by a straight line that *goes through our data*, which is called linear regression. \n",
    "\n",
    "Given a feature or measurement $x$ and another measurement $y$ we want to find the parameters of a line such that $y = f(x) = a \\cdot x+b$. The goal of regression is to find optimal values for $a$ and $b$ so that we can predict the value for $y$. Usually we call these parameters **weights** and use the $w$ abbreviation so that $y = w_0 + w_1 \\cdot x$.\n",
    "\n",
    "Let's look at two variables from the wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cdd82-a497-4071-afcb-f98330faa73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "sns.set_theme(style='white', palette='deep')\n",
    "\n",
    "# Make plots somewhat interactive - requires ipympl to be installed\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8539ec-1a73-4eba-b304-5cf59a379dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430ea75-acc3-49f1-b6f9-1bfd234c737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9653a4e-9a72-4360-9779-47f3b5662359",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad63884-4dc8-4cc0-a7d5-89c55e4414e0",
   "metadata": {},
   "source": [
    "We see that there is a correlation between `density` and `fixed acidity`. We can try to find that actual relation and first assume that: `fixed acidity` = $w_0$ + $w_1 \\cdot$ `density`. What we need to do is to find the optimal values for $w_0$ and $w_1$. This is the simplest linear \"combination\" that we can do: a fixed value, the bias $w_0$ and a parameter that multiplies the value of one feature.\n",
    "\n",
    "Let's plot the result for some random choice of parameters e.g. $w_0=-90, w_1=100$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a3ba3-696d-4dad-a8b3-a5e838bde678",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity', ax=ax);\n",
    "plt.plot(wine['density'], -90 + 100 * wine['density'], 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c24cd-e2c5-43fd-aac8-54a5ac17c83e",
   "metadata": {},
   "source": [
    "## Estimating the error\n",
    "\n",
    "We see that we are quite far away from the best solution. But how do we even define what the best solution is? We can do that by estimating the error that our model makes on average. Imagine that we measure the distance from each data point to the red line. The higher the distance, the higher the error. These distances are usually called *residuals*. In the end, we want a red line, that minimizes this error on average. Of course we will not be able to get our error down to zero, but we want to get as close as possible to it.\n",
    "\n",
    "How can we calculate the residuals? Well we just need to subtract the predicted values from the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd763b-8102-4e8d-909e-58f0cae6a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = wine['fixed acidity']\n",
    "predicted_values = -90 + 100 * wine['density']\n",
    "distance_to_model = actual_values - predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5d561",
   "metadata": {},
   "source": [
    "The `distance_to_model` array corresponds to the red dotted distances in the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x=\"density\", y=\"fixed acidity\")\n",
    "plt.plot(wine[\"density\"], predicted_values, \"r\")\n",
    "plt.vlines(\n",
    "    wine[\"density\"], wine[\"fixed acidity\"], predicted_values, \"crimson\", linestyle=\"dotted\", alpha=0.2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813fc39-2bee-4dad-9b3e-be01b3936d33",
   "metadata": {},
   "source": [
    "Let's plot these distances/errors as a scatterplot to understand them better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3dcd8-d729-4306-8f33-f635fdc879d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = sns.scatterplot(x=wine['density'],  y=distance_to_model, marker='x', color='red', ax=ax)\n",
    "ax.set_ylabel('distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439b04e-d6c2-491d-ba2b-63282c1e0dd7",
   "metadata": {},
   "source": [
    "We see that we get some negative values on the left when the model predicts higher values than expected and positive when the model is lower than expected on the right. Let's just try to find a slightly better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5e397-6611-4386-80e9-33c205e1dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity');\n",
    "plt.plot(wine['density'], -589 + 600 * wine['density'], 'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e3416-b16d-4cc6-ba4f-877385f49fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_model = wine['fixed acidity'] - (-589 + 600 * wine['density'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.scatterplot(x=wine['density'],  y=distance_to_model, marker='x', color='red', ax=ax)\n",
    "ax.set_ylabel('distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6c4ab-9ebd-4394-a4b9-3b5ae5b1b876",
   "metadata": {},
   "source": [
    "This looks better, but we still need a better way of *quantifying* the quality of the model. We cannot just look at such error plots by eye. Of course, we can sum up all the individual residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534fefc-52b0-4f6e-859c-55e24da2c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error = np.sum(distance_to_model)\n",
    "total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d725e-be01-471c-af80-5b8b7c11cc1c",
   "metadata": {},
   "source": [
    "Does this make sense? Let's look at a histogram of our distance values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c35d3-0509-436a-8868-05402a16658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(distance_to_model).set(xlabel=\"Error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468396b-7b0d-4cec-84f9-8387a1c6bc6b",
   "metadata": {},
   "source": [
    "If we just take the three largest bins, we should already have a total error of about -1000, so the error seems largely underestimated. The problem is that we are interested in the absolute distances of the model predictions to the actual values. We don't really care if they are larger or smaller. When we just sum up the distance, the negative and positive errors cancel each other out, leading to an underestimation. \n",
    "\n",
    "In the worst case, our model predicts values that are too high for half of the samples and values that are too low for the other half. In tbhis case, the error might appear to be close to zero. What we need is an estimation of the absolute error. We could take the absolute value $abs(x)$, but generally what is done is to just take the square of the errors. Now a distance of -3 or +3 always has an error of 9 as $3^2 = -3^2$. Additionally, this *punishes* larger errors much more than smaller errors.\n",
    "\n",
    "Let's compute the sum of the squared distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023962a-210f-4aef-9d2d-971e7e5d4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_error = np.sum(distance_to_model**2)\n",
    "square_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8a939-9653-4b2b-a414-d01544dd58fb",
   "metadata": {},
   "source": [
    "## Finding the best solution\n",
    "\n",
    "In most cases, we will find the best values for the parameters $w_0$ and $w_1$ through an iterative process: we start from a random point and then try to minimize the error of our model. In a simple case like ours, we could just make a grid of possible values for $w_0$ and $w_1$ and check which are the best. Below you can see how this is done. The code might be quite complicated at first to understand, so here is an explanation of what it does:\n",
    "\n",
    "1. Create ranges for the different $w_0$ and $w_1$ that we want to test\n",
    "2. Create a meshgrid from the two ranges. A meshgrid is a way to generate coordinate grids from two 1D arrays, producing 2D arrays that represent all combinations of those values. It’s commonly used to evaluate functions over a grid without using nested loops (vectorization).\n",
    "3. Compute the predicted `fixed acidity` value for all of the $w_0$ and $w_1$ combinations in the grid.\n",
    "4. Calculate the errors for each classifier (each combination of $w_0$ and $w_1$).\n",
    "5. Find the indices in the grid (corresponding to a specifc pair of $w_1$ and $w_2$ parameter values) that produced the smallest error.\n",
    "6. Plot the errors error surface using a heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c75d9a-207d-4fa2-b76e-c6c6548ae968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify the different ranges that we want to explore\n",
    "w0_range = np.arange(-1000, 0, 5)\n",
    "w1_range = np.arange(0, 1000, 5)\n",
    "\n",
    "# 2. Create a grid of values for the parameters\n",
    "grid_0, grid_1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# 3. Predict the fixed acidity values for each combination of parameters (w0, w1)\n",
    "grid_mult = grid_0[:,:,np.newaxis] + grid_1[:,:,np.newaxis] * wine['density'].values\n",
    "\n",
    "# 4. Compute the squared error for each combination of parameters\n",
    "grid_mult = (grid_mult - wine['fixed acidity'].values)**2\n",
    "total_error = np.sum(grid_mult, axis=2)\n",
    "\n",
    "# 5. Find the minimal error\n",
    "min_loc = np.unravel_index(np.argmin(total_error), total_error.shape)\n",
    "\n",
    "# 6. Plot the error surface\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(total_error, extent=[w0_range[0],w0_range[-1],w1_range[-1],w1_range[0]],cmap=cm.coolwarm)\n",
    "ax.plot([grid_0[min_loc[0],min_loc[1]]], [grid_1[min_loc[0],min_loc[1]]], 'y*', markersize=10)\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$');\n",
    "fig.colorbar(im, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f416f1-9135-41b5-9867-43f3f1bf48db",
   "metadata": {},
   "source": [
    "In this plot we see the error of the model (how well it goes through the points) for each pair of values $w_0$, $w_1$. Dark blue represents a better model while red represents models that perform worse. The yellow star indicates the best possible solution with values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8ad9b-c08c-4316-ba1c-232e53aaf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = grid_0[min_loc[0],min_loc[1]]\n",
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bafa7-b0f6-4d48-b6a4-702599452240",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = grid_1[min_loc[0],min_loc[1]]\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062ad03-a722-4e0c-85e9-44d03fa2404a",
   "metadata": {},
   "source": [
    "We can now use this values in our equation $y=w_0 + w_1*x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b793c0-811f-4a40-9ae7-9f70c4fe8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity');\n",
    "\n",
    "densities = np.arange(wine.density.min(), wine.density.max(), 0.001)\n",
    "plt.plot(densities, w0 + densities * w1, 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96032d5-8536-4378-bf9b-8171deea5b87",
   "metadata": {},
   "source": [
    "Usually we cannot directly compute the optimal value (yellow star location) because it would take way too much time to compute the error for all combinations of parameters. In that case we would start from a random point and search for the solution in an *iterative* way. We can represent the error plot in 3D to understand better. Let's assume we start from the location of the green dot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e3ba3-b349-4338-a9df-7ee5fbeffc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "X = grid_0.copy()\n",
    "Y = grid_1.copy()\n",
    "Z = total_error.copy()\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d',computed_zorder=False)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)#, vmax=5)\n",
    "\n",
    "# Add green dot and yellow star to indicate start and goal\n",
    "ax.scatter([grid_0[180, 150]],[grid_1[180, 150]], [total_error[180, 150]], c='green', marker='o', s=50)\n",
    "ax.scatter([grid_0[min_loc[0],min_loc[1]]],[grid_1[min_loc[0],min_loc[1]]], [total_error[min_loc[0],min_loc[1]]], c='yellow', marker=\"*\", s=50)\n",
    "\n",
    "ax.set_xlabel('$w_1$', fontdict={'fontsize':20})\n",
    "ax.set_ylabel('$w_0$', fontdict={'fontsize':20});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd6700-3ba9-432a-af7e-4e0a843fd8dc",
   "metadata": {},
   "source": [
    "We can now imagine taking many small steps downhill. Eventually, we would reach the bottom of the blue valley, where the yellow star is, which would correspond to the best value. This approach is very common in ML and is called gradient descent, gradient being a synonym for slope. The idea of the algorithm is to follow the steepest slope to find the minimal error!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40808073-d136-40ce-8e19-58ec37c6b1d2",
   "metadata": {},
   "source": [
    "## Finding the best solution with scikit-learn\n",
    "\n",
    "In reality, we don't have to perform all these steps ourselves, we can use algorithm that have been implemented and optimized in libraries like scikit-learn. The simplest such algorithm is just called ```LinearRegression``` and is available in the submodule ```linear_model``` (technically, ```LinearRegression``` doesn't use gradient descent but ```LinearRegressionSGD``` does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e982a-5df2-4fd0-9592-68afd4fed7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32114c7a-db3b-4869-ab83-1b066d50e75d",
   "metadata": {},
   "source": [
    "A large part of the algorithms implemented in scikit-learn in a similar way:\n",
    "\n",
    "1. Create arrays containing the independent ($X$) and dependent ($y$) variables\n",
    "2. Instantiate a chosen model, e.g. `LinearRegression` with some optional parameters specific to the method\n",
    "3. \"Train\" or \"fit\" the model, i.e. find the optimal parameters for your dataset\n",
    "4. Use the trained model for predictions\n",
    "\n",
    "In our current example, the independent variable is just the ```density``` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d15d81-9a57-4e7f-9a18-1fa40e421dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine[['density']].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112b20a-f2cd-41ae-87fb-20d9f4f0ac98",
   "metadata": {},
   "source": [
    "Note that we pass a **list** of columns even though we have a single column. If we do not do this, then the returned $X$ has a single dimension but sklearn expects in general 2D objects (tables/matrices). The reason behind this is that you would usually use multiple features for $X$, in which case you would actually select multiple columns from your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1186976-94d8-48da-9e5d-5327ed87e986",
   "metadata": {},
   "source": [
    "The variable that we want to precit can be a single vector, therefore we can just set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b444c-d416-4898-a0d9-cd0535559309",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wine['fixed acidity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff0839-88a1-4465-a44e-f851944f1d01",
   "metadata": {},
   "source": [
    "Now we instantiate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f62aa-d767-459b-8805-f5388c908ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fdd82b-5736-412f-a7a3-511ec15ca42f",
   "metadata": {},
   "source": [
    "Finally, we call the ```fit``` method using our *training data* $X$ and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe68754-23c9-4980-9319-372a9614bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe4d9f-c07f-43c1-9428-a32cc18f4671",
   "metadata": {},
   "source": [
    "The ```res``` object contains all the information about our fit. In particular it contains the values of the parameters in ```coef_```, which corresponds to $w_1$ in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ec8d4-5a44-495c-8328-7318aea3e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64854197-608f-40a1-90c4-d0a336a29885",
   "metadata": {},
   "source": [
    "To get the first term $w_0$ we need to look up the  intercept, which can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102a586-3eda-45d8-9c2c-6617a9e535d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6081df1-51a9-4794-b23c-7f756bce4f37",
   "metadata": {},
   "source": [
    "Now that we have model we can make a **prediction**, i.e. guess the value of $y$ (here fixed acidity) given a specific value x (here density). Let's try that for $x=0.996$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8c379-3cd6-4ce4-ad76-027efba56238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[0.996]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe04374-3d98-4795-ad9f-3d615751afce",
   "metadata": {},
   "source": [
    "Note that here again, to do this prediction we use a list of lists ```[[0.996]]``` even for a single number. We trained with a 2D object, so we predict with a 2D object as well. Of course, we can do a prediction for an entire series of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd058044-e100-45f4-9ad3-42ccf151dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.arange(0.99, 1.004, 0.001)\n",
    "X_pred = X_pred[:, np.newaxis]\n",
    "X_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333548c1-cd31-414f-a4f4-28ffb94be3ce",
   "metadata": {},
   "source": [
    "Note that here we use ```np.newaxis``` to transform again our 1D list genereated by ```np.arange``` into 2D data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a248a0-d521-4537-ba57-fcb8740f59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdd5d3-d22d-4739-a947-f1673bd785a0",
   "metadata": {},
   "source": [
    "Naturally, we can plot again the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8130fd-d6d1-492a-9c2f-5cb0091db7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=wine, x='density', y='fixed acidity');\n",
    "plt.plot(X_pred, y_pred, 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b7b13-477d-42db-afb5-f35c2097bed9",
   "metadata": {},
   "source": [
    "We see that the values that we get are close to our approximation above. It we were using a finer grid for our approximation we would end up with the same values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b5bbe-cd0c-49a9-b6b7-33fcc61dac27",
   "metadata": {},
   "source": [
    "## Measuring quality with scikit-learn\n",
    "\n",
    "Above, we have always manually computed an error e.g. as the sum or mean of squared distances between points and their predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc3aae-2066-47c5-b7a4-3d0e2c72edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = model.predict(X)\n",
    "\n",
    "np.mean((y_train -y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ff9a3-27a1-4ae4-9264-4ce94d373e4c",
   "metadata": {},
   "source": [
    "Instead of doing this manually with the risk of making a mistake, we can also use function from scikit-learn. Those functions compute so-called metrics, such as the mean squared error and are located in the ```metrics``` submodule of scikit-learn. We will see such metrics in other contexts, especially classification, and they always require the true and predicted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0758fa9-b972-48b6-a222-386eced889b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c61e60-4bfb-4f9e-afa4-0849d5f07d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff25ef1-d901-4084-ab4f-897217279906",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Import the co2 dataset called `co2_countries` from the dataset directory. This dataset contains information C02 emissions between 1990-2022 of Germany and France\n",
    "\n",
    "2. With seaborn make a scatter plot of ```co2_per_capita``` vs ```Year```. Color data by country.\n",
    "\n",
    "3. Using scikit learn's ```LinearRegression``` fit a model to the relation between ```co2_per_capita``` and ```Year``` for Germany and France **separately**.\n",
    "\n",
    "4. Plot the scatterplot and the fitted linear models on top of it.\n",
    "\n",
    "5. Which country can be better described by this linear model? Compute the mean square error of the two models.\n",
    "\n",
    "6. What is the emission per capita predicted for France in 2030?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAI3-2025-Dx0OIx0W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
